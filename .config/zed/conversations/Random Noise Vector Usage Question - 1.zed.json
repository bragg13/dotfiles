{"id":"b4e885cc-ca2c-4625-88ff-22eaf232a868","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs/simple_autoencoder/train.py:1-140\n# Excerpt from: \n\"\"\"Training and evaluation logic.\"\"\"\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom absl import logging\nfrom flax import linen as nn\nimport nsd_data\nimport models\nfrom logger import log\nfrom visualisations import plot_first_100vx_over_epochs, plot_losses, plot_results_before_after_training, plot_results_epoch\nimport utils as vae_utils\nfrom flax.training import train_state, checkpoints\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport ml_collections\nimport optax\nimport tensorflow_datasets as tfds\nfrom tqdm import tqdm\nfrom nsd_data import get_train_test_datasets, get_batches\n\ndef compute_metrics(recon_x, x):\n    loss = jnp.mean(jnp.square(recon_x - x))\n    return {'loss': loss }\n\ndef train_step(state, batch, z_rng, latent_dim, l1_coefficient=0.01):\n    def loss_fn(params):\n        fmri_voxels = batch.shape[1]\n        # print(batch.shape)\n        recon_x, latent_vec = models.model(latent_dim, fmri_voxels).apply(\n            {'params': params}, batch, z_rng\n        )\n        mse_loss = jnp.mean(jnp.square(recon_x - batch))\n        sparsity_loss = l1_coefficient * jnp.sum(latent_vec) # jnp.sum(jnp.abs(latent_vec))\n\n        return mse_loss\n\n    grads = jax.grad(loss_fn)(state.params)\n    return state.apply_gradients(grads=grads)\n\ndef eval_f(params, batch, z, z_rng, latent_dim):\n    def eval_model(ae):\n        recon_x, latent_vecs = ae(batch, z_rng)\n        # log(f'dim reconx: {recon_x.shape}', 'EVAL_F')\n        metrics = compute_metrics(recon_x, batch)\n        return metrics, (batch, recon_x), latent_vecs\n\n    fmri_voxels = batch.shape[1]\n    return nn.apply(eval_model, models.model(latent_dim, fmri_voxels))({'params': params})\n\ndef train_and_evaluate(config):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    log('Initializing dataset...', 'TRAIN')\n    train_ds, test_ds = get_train_test_datasets(subject=3, roi_class='floc-bodies', hem='lh')\n    print(f\"training ds shape: {train_ds.shape}\")\n    print(f\"test ds shape: {test_ds.shape}\")\n    train_loader = get_batches(train_ds, config.batch_size)\n    fmri_voxels = train_ds.shape[1]\n\n    log('Initializing model...', 'TRAIN')\n    # va bene che siano uni o random gaussian noise?\n    init_data = jnp.ones((config.batch_size, fmri_voxels), jnp.float32)  # fmri of 4 shown-images, 2000 voxels each\n\n    log('Initializing params...', 'TRAIN')\n    params = models.model(config.latent_dim, fmri_voxels).init(key, init_data, rng)['params']\n\n    log('Initializing state...', 'TRAIN')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latent_dim, fmri_voxels).apply,\n        params=params,\n        tx=optax.adamw(config.learning_rate),\n    )\n\n    rng, z_key, eval_rng = random.split(rng, 3)\n    z = random.normal(z_key, (64, config.latent_dim))\n\n    log(f'Calculating training steps per epochs (train_size: {train_size})...', 'TRAIN')\n    steps_per_epoch = train_size // int(config.batch_size)\n    if train_size % int(config.batch_size) != 0:\n        steps_per_epoch += 1\n    log(f\"{steps_per_epoch} steps for each ({config.num_epochs}) epoch\", 'TRAIN')\n\n    log('Collecting test batches...', 'TRAIN')\n    test_ds = iter(test_loader)\n    test_batches = []\n    for step, batch in enumerate(test_loader):\n        test_batches.append(batch)\n    log(f'collected {len(test_batches)} test batches', 'TRAIN')\n\n    log('\\nstarting training', 'TRAIN')\n    losses = []\n\n    data_to_reconstruct = test_batches[0]\n    all_reconstructions = []\n    all_reconstructions.append(data_to_reconstruct)\n\n    for epoch in range(config.num_epochs):\n        log(f'Epoch {epoch + 1}/{config.num_epochs}', 'TRAIN LOOP')\n\n        # Training loop\n        for step, batch in tqdm(enumerate(train_loader), total=steps_per_epoch):\n            if step >= steps_per_epoch:\n                break\n\n            rng, key = random.split(rng)\n            state = train_step(state, batch, key, config.latent_dim)\n            metrics, (_batch, reconstructions), latent_vecs = eval_f(\n                state.params, batch, z, eval_rng, config.latent_dim\n            )\n            losses.append(metrics['loss'])\n            plot_results_epoch(_batch, reconstructions, latent_vecs, epoch)\n\n        # eval\n        # metrics, (_batch, reconstructions), latent_vecs = eval_f(\n        #     state.params, batch, z, eval_rng, config.latent_dim\n        # )\n        # metrics, (batch, reconstructions), latent_vecs = eval_f(\n        #     state.params, test_batches[epoch], z, eval_rng, config.latent_dim\n        # )\n        # print(\n        #     'eval epoch: {}, loss: {:.4f}'.format(\n        #         epoch + 1, metrics['loss']\n        #     )\n        # )\n        # losses.append(metrics['loss'])\n\n        # for evolution visualisation\n        # print(f\"data to rec has shape {data_to_reconstruct.shape} \")\n        # metrics, (batch, reconstructions), latent_vecs = eval_f(\n        #     state.params, data_to_reconstruct, z, eval_rng, config.latent_dim\n        # )\n        # print(f\" rec has shape {reconstructions.shape} \")\n        # all_reconstructions.append(reconstructions)\n\n    # plot_first_100vx_over_epochs(np.array(all_reconstructions))\n    # plot_results_before_after_training(batch reconstructions, latent_vec)\n    plot_losses(losses)\n\n```\ni dont understand how `z` in line 63 is used.\nLooking at the code, `z` is created as a random normal array but isn't actually used in any meaningful way in the rest of the code. It's passed to `eval_f()` but never used within that function.\n\nHere's the line creating `z`:\n```python\nz = random.normal(z_key, (64, config.latent_dim))\n```\n\nAnd while it's passed to `eval_f()`:\n```python\nmetrics, (_batch, reconstructions), latent_vecs = eval_f(\n    state.params, batch, z, eval_rng, config.latent_dim\n)\n```\n\nThe `eval_f()` function takes it as a parameter but never uses it:\n```python\ndef eval_f(params, batch, z, z_rng, latent_dim):  # z is unused\n    def eval_model(ae):\n        recon_x, latent_vecs = ae(batch, z_rng)\n        metrics = compute_metrics(recon_x, batch)\n        return metrics, (batch, recon_x), latent_vecs\n\n    fmri_voxels = batch.shape[1]\n    return nn.apply(eval_model, models.model(latent_dim, fmri_voxels))({'params': params})\n```\n\nThis appears to be either:\n1. Dead code that was left over from previous development\n2. Code that was intended for future use but never implemented\n3. A mistake in the implementation\n\nThe `z` variable seems to be intended to represent some latent space vectors, possibly for a variational autoencoder, but it's not actually being used in the current implementation. You could safely remove this variable and its related parameter passing without affecting the functionality of the code.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":89}}},{"id":{"replica_id":0,"value":48},"start":5389,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":92}}},{"id":{"replica_id":0,"value":49},"start":6782,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":91}}}],"summary":"Random Noise Vector Usage Question","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}