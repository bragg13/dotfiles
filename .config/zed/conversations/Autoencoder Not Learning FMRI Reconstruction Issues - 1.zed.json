{"id":"ea60b62d-a9ef-4c4e-92c1-619b11e02641","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs/simple_autoencoder/models.py:1-59\n# Excerpt from: \n\n\"\"\"VAE model definitions.\"\"\"\n\nfrom flax import linen as nn\nfrom jax import random\nimport jax.numpy as jnp\n\nclass Encoder(nn.Module):\n  \"\"\"AE Encoder.\"\"\"\n\n  latent_dim: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(2048, name='fc1')(x)\n    x = nn.relu(x)\n    x = nn.Dense(1024, name='fc2')(x)\n    x = nn.relu(x)\n    x = nn.Dense(self.latent_dim, name='fc3')(x)\n    x = nn.relu(x)\n    return x\n\nclass Decoder(nn.Module):\n  \"\"\"AE Decoder.\"\"\"\n\n  fmri_dim: int\n\n  @nn.compact\n  def __call__(self, z):\n    z = nn.Dense(1024, name='fc1')(z)\n    z = nn.relu(z)\n    z = nn.Dense(2048, name='fc2')(z)\n    z = nn.relu(z)\n    z = nn.Dense(self.fmri_dim, name='fc3')(z)\n    # we dont need sigmoid\n    return z\n\n\nclass AE(nn.Module):\n  \"\"\"Full AE model.\"\"\"\n\n  latent_dim: int #= 3000\n  fmri_dim: int # = 3633 #7266\n\n  def setup(self):\n    self.encoder = Encoder(self.latent_dim)\n    self.decoder = Decoder(self.fmri_dim)\n\n  def __call__(self, x, z_rng):\n    latent_vec = self.encoder(x)\n    recon_x = self.decoder(latent_vec)\n    return recon_x, latent_vec\n\n  def generate_fmri_from_latent_vec(self, z):\n    return self.decoder(z)\n\ndef model(latent_dim, fmri_dim):\n  return AE(latent_dim=latent_dim, fmri_dim=fmri_dim)\n\n```\n\n```py aigs/simple_autoencoder/train.py:1-140\n# Excerpt from: \n\"\"\"Training and evaluation logic.\"\"\"\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom flax import linen as nn\nimport nsd_data\nimport models\nfrom logger import log\nimport utils as vae_utils\nfrom flax.training import train_state, checkpoints\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport ml_collections\nimport optax\nimport tensorflow_datasets as tfds\nfrom tqdm import tqdm\n\ndef compute_metrics(x, recon_x):\n    loss = jnp.mean(jnp.square(x - recon_x))\n    return {'loss': loss }\n\ndef train_step(state, batch, z_rng, latent_dim, l1_coefficient=0.01):\n    def loss_fn(params):\n        fmri_voxels = batch.shape[1]\n        recon_x, latent_vec = models.model(latent_dim, fmri_voxels).apply(\n            {'params': params}, batch, z_rng\n        )\n\n        # loss is composed of reconstruction and sparsity loss,\n        # where the sparsity loss uses a l1 coefficient to punish the latent rep activations or sth like that\n        mse_loss = jnp.mean(jnp.square(recon_x - batch))\n        # sparsity_loss = l1_coefficient * jnp.sum(latent_vec)\n\n        # can also be impemented as KL-divergence\n        # mean_activation = jnp.mean(hidden_activations, axis=0)\n        # kl_divergence = jnp.sum(\n        #        sparsity_level * jnp.log(sparsity_level / (mean_activation + 1e-10)) +\n        #        (1 - sparsity_level) * jnp.log((1 - sparsity_level) / (1 - mean_activation + 1e-10))\n        #    )\n        return mse_loss #+ sparsity_loss\n\n    grads = jax.grad(loss_fn)(state.params)\n    return state.apply_gradients(grads=grads)\n\ndef eval_f(params, batch, z_rng, latents):\n    def eval_model(ae):\n        recon_x, latent_vec = ae(batch, z_rng)\n\n        # get first 100 voxels of the original fmri and reconstructed one\n        # NOW: only of the first element in the batch\n        # TODO: for all the bathc using etsch visualisation\n        recon_as_image = recon_x[:, :100]\n        original_as_image = batch[:, :100]\n        difference = original_as_image-recon_as_image\n\n        metrics = compute_metrics(batch, recon_x)\n        return metrics, difference\n\n    fmri_voxels = batch.shape[1]\n    return nn.apply(eval_model, models.model(latents, fmri_voxels))({'params': params})\n\ndef train_and_evaluate(config):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    log('Initializing dataset...', 'TRAIN')\n    idxs = nsd_data.split_idxs()\n    subject_idxs = (idxs['subject_train'], idxs['subject_test'])\n    train_loader, test_loader, train_size, test_size, fmri_voxels = nsd_data.create_loaders(subject_idxs, roi=config.roi, hem=config.hem, batch_size=config.batch_size)\n\n    log('Initializing model...', 'TRAIN')\n    init_data = jnp.ones((config.batch_size, fmri_voxels), jnp.float32)\n\n    log('Initializing params...', 'TRAIN')\n    params = models.model(config.latent_dim, fmri_voxels).init(key, init_data, rng)['params']\n\n    log('Initializing state...', 'TRAIN')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latent_dim, fmri_voxels).apply, # calls the fun __call__ in model\n        params=params,\n        tx=optax.adam(config.learning_rate),\n    )\n    rng, z_key, eval_rng = random.split(rng, 3)\n    z = random.normal(z_key, (64, config.latent_dim)) # test size?\n\n    log('Calculating training steps per epochs...', 'TRAIN')\n    steps_per_epoch = train_size // int(config.batch_size)\n    if train_size % int(config.batch_size) != 0:\n        steps_per_epoch += 1\n\n    # i only need one batch to test agains each epoch\n    log('Gathering testing batches...', 'TRAIN')\n    test_ds = iter(test_loader)\n    test_batch = next(test_ds)\n\n    # test_batches = []\n    # for i, batch in enumerate(test_loader):\n    #     if i >= config.epochs:\n    #         break\n    #     test_batches.append(batch)\n\n\n    log('\\nstarting training', 'TRAIN')\n    for epoch in range(config.num_epochs):\n        log(f'Epoch {epoch + 1}/{config.num_epochs}', 'TRAIN LOOP')\n\n        # Training loop\n        epoch_loss = 0.0\n        for step, batch in tqdm(enumerate(train_loader), total=steps_per_epoch, desc=f'Training epoch {epoch + 1}'):\n            if step >= steps_per_epoch:\n                break\n\n            rng, key = random.split(rng)\n            state = train_step(state=state, batch=batch, z_rng=key, latent_dim=config.latent_dim)\n\n        # eval - always against the same test batch\n        metrics, difference = eval_f(\n            params=state.params, batch=test_batch, z_rng=eval_rng, latents=config.latent_dim\n        )\n\n        fig, axs = plt.subplots(figsize=(4, 8))\n        axs.imshow(difference, cmap='gray')\n        axs.set_title('Original fMRI + reconstruction')\n        fig.savefig(f'results/difference_{epoch}.png')\n        plt.close()\n\n        # fig2, axs2 = plt.subplots(figsize=(4, 8))\n        # axs2.plot(latent_vec)\n        # print(latent_vec)\n        # axs2.set_title('latent vector View')\n        # fig2.savefig(f'results/latent_vec_{epoch}.png')\n        # plt.close()\n\n        print(\n            'eval epoch: {}, loss: {:.4f}'.format(\n                epoch + 1, metrics['loss']\n            )\n        )\n\n```\nthis is my autoencoder model to reconstruct FMRI data. options: --learning_rate 0.0001 --latent_dim 30 --batch_size 30 --num_epochs 15 --roi floc-bodies --hem lh.\nit is not learning almost at all. the loss passes from 0.37 to 0.32 in 15 epochs.\nwhat is the problems?\n\nThere are several potential issues that might be affecting your autoencoder's learning performance:\n\n1. **Architecture Related**:\n- The network might be too deep or too wide for your data. You're using large dense layers (2048 -> 1024 -> latent_dim) which might be overkill and make learning harder\n- You're using ReLU activation after the encoder's final layer, which might be limiting the latent space representation\n- No normalization layers (BatchNorm or LayerNorm) which could help with training stability\n\n2. **Training Related**:\n- The learning rate (0.0001) might be too low for your model\n- The batch size (30) is relatively small which could lead to noisy gradients\n- No learning rate scheduling is being used\n- The latent dimension (30) might be too small to capture all the necessary information from fMRI data\n\n3. **Data Related**:\n- No data normalization/standardization is visible in the code\n- No data augmentation\n- The L1 regularization is commented out, which could help with sparsity\n\nHere are some suggestions to improve the model:\n\n```python\nclass Encoder(nn.Module):\n    latent_dim: int\n    \n    @nn.compact\n    def __call__(self, x, training=True):\n        x = nn.Dense(1024)(x)  # Reduced size\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        \n        x = nn.Dense(512)(x)   # Reduced size\n        x = nn.BatchNorm(use_running_average=not training)(x)\n        x = nn.relu(x)\n        \n        x = nn.Dense(self.latent_dim)(x)\n        # Remove ReLU from final layer\n        return x\n\nclass Decoder(nn.Module):\n    fmri_dim: int\n    \n    @nn.compact\n    def __call__(self, z, training=True):\n        z = nn.Dense(512)(z)\n        z = nn.BatchNorm(use_running_average=not training)(z)\n        z = nn.relu(z)\n        \n        z = nn.Dense(1024)(z)\n        z = nn.BatchNorm(use_running_average=not training)(z)\n        z = nn.relu(z)\n        \n        z = nn.Dense(self.fmri_dim)(z)\n        return z\n```\n\nAnd modify your training configuration:\n\n```python\nconfig.learning_rate = 0.001  # Increased learning rate\nconfig.batch_size = 64        # Increased batch size\nconfig.latent_dim = 64       # Increased latent dimension\n\n# Add learning rate scheduling\nscheduler = optax.exponential_decay(\n    init_value=config.learning_rate,\n    transition_steps=1000,\n    decay_rate=0.9\n)\ntx = optax.chain(\n    optax.clip_by_global_norm(1.0),  # Gradient clipping\n    optax.adam(learning_rate=scheduler)\n)\n```\n\nAlso, make sure to normalize your fMRI data before training:\n\n```python\n# In your data loading/preprocessing\nfmri_data = (fmri_data - np.mean(fmri_data)) / np.std(fmri_data)\n```\n\nTry these modifications and see if they improve the learning performance. You might need to experiment with different hyperparameters to find the optimal configuration for your specific case.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":175}}},{"id":{"replica_id":0,"value":154},"start":6740,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":176}}},{"id":{"replica_id":0,"value":155},"start":9571,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":177}}}],"summary":"Autoencoder Not Learning FMRI Reconstruction Issues","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}