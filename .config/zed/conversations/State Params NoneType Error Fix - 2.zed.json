{"id":"107afb63-933e-4209-8fbc-dd86044bf9c0","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs-nsd-reconstruction/simple_autoencoder/train.py:1-114\n# Excerpt from: \n\"\"\"Training and evaluation logic.\"\"\"\n\nfrom absl import logging\nfrom flax import linen as nn\nimport input_pipeline\nimport nsd_data\nimport models\nimport utils as vae_utils\nfrom flax.training import train_state, checkpoints\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport ml_collections\nimport optax\nimport tensorflow_datasets as tfds\n\n# @jax.vmap\n# def binary_cross_entropy_with_logits(logits, labels):\n#   logits = nn.log_sigmoid(logits)\n#   return -jnp.sum(\n#       labels * logits + (1.0 - labels) * jnp.log(-jnp.expm1(logits))\n#   )\n\n\ndef compute_metrics(recon_x, x):\n    # loss = binary_cross_entropy_with_logits(recon_x, x).mean()\n    loss = jnp.mean(jnp.square(recon_x - x))\n    return {'loss': loss }\n\n\ndef train_step(state, batch, z_rng, latents):\n    def loss_fn(params):\n        recon_x = models.model(latents).apply(\n            {'params': params}, batch, z_rng\n        )\n\n        loss = jnp.mean(jnp.square(recon_x - batch))\n        # loss = binary_cross_entropy_with_logits(recon_x, batch).mean()\n        return loss\n\n        grads = jax.grad(loss_fn)(state.params)\n        return state.apply_gradients(grads=grads)\n\n\ndef eval_f(params, images, z, z_rng, latents):\n    def eval_model(ae):\n        recon_images = ae(images, z_rng)\n        comparison = jnp.concatenate([\n            images[:8].reshape(-1, 28, 28, 1),\n            recon_images[:8].reshape(-1, 28, 28, 1),\n        ])\n\n        generate_images = ae.generate(z)\n        generate_images = generate_images.reshape(-1, 28, 28, 1)\n        metrics = compute_metrics(recon_images, images)\n        return metrics, comparison, generate_images\n\n    return nn.apply(eval_model, models.model(latents))({'params': params})\n\n\ndef train_and_evaluate(config: ml_collections.ConfigDict):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    print('Initializing dataset.')\n    idxs = nsd_data.shuffle_idxs()\n    train_loader, test_loader = nsd_data.create_loaders(idxs, roi=None, batch_size=config.batch_size)\n    train_ds = iter(train_loader)\n    test_ds = iter(test_loader)\n\n    print('Initializing model.')\n    init_data = jnp.ones((config.batch_size, 4, 2000), jnp.float32)  # fmri of 4 shown-images, 2000 voxels each\n\n    print('initialising params')\n    params = models.model(config.latents).init(key, init_data, rng)['params']\n\n    print('Initializing state')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latents).apply,\n        params=params,\n        tx=optax.adam(config.learning_rate),\n    )\n    print(state)\n\n    rng, z_key, eval_rng = random.split(rng, 3)\n    z = random.normal(z_key, (64, config.latents))\n\n        # ds_builder.info.splits['train'].num_examples\n    number_images = 600\n    steps_per_epoch = (number_images // int(config.batch_size)) # n examples)\n\n    print('starting training')\n    for epoch in range(config.num_epochs):\n        for s in range(steps_per_epoch):\n            # images, lh_fmri, rh_fmri = next(train_ds)\n            batch = next(train_ds)\n            rng, key = random.split(rng)\n            state = train_step(state, batch, key, config.latents)\n\n            metrics, comparison, sample = eval_f(\n                state.params, test_ds, z, eval_rng, config.latents\n            )\n            vae_utils.save_image(\n                comparison, f'results/reconstruction_{epoch}.png', nrow=8\n            )\n            vae_utils.save_image(sample, f'results/sample_{epoch}.png', nrow=8)\n\n            print(\n                'eval epoch: {}, loss: {:.4f}'.format(\n                    epoch + 1, metrics['loss']\n                )\n            )\n\n```\n i get line 102, in train_and_evaluate\n     state.params, test_ds, z, eval_rng, config.latents\n     ^^^^^^^^^^^^\n AttributeError: 'NoneType' object has no attribute 'params'\n \nThe error occurs because `train_step` is not returning the updated state. In your current code, the `train_step` function has incorrect indentation and return statement. Here's the corrected version:\n\n```python\ndef train_step(state, batch, z_rng, latents):\n    def loss_fn(params):\n        recon_x = models.model(latents).apply(\n            {'params': params}, batch, z_rng\n        )\n\n        loss = jnp.mean(jnp.square(recon_x - batch))\n        # loss = binary_cross_entropy_with_logits(recon_x, batch).mean()\n        return loss\n\n    grads = jax.grad(loss_fn)(state.params)\n    return state.apply_gradients(grads=grads)  # Return the updated state\n```\n\nThe key issues that were fixed:\n\n1. The `grads = jax.grad(loss_fn)(state.params)` line was inside the `loss_fn` function, which was incorrect\n2. The `return state.apply_gradients(grads=grads)` line was also indented incorrectly\n3. The function wasn't returning anything, which caused `state` to become `None`\n\nNow the `train_step` function properly:\n1. Computes the loss function\n2. Calculates the gradients\n3. Applies the gradients to update the state\n4. Returns the updated state\n\nThis should resolve the NoneType error you're encountering. The state will now be properly updated and maintained throughout training.\n\"\"\"Training and evaluation logic.\"\"\"\n\nfrom absl import logging\nfrom flax import linen as nn\nimport input_pipeline\nimport nsd_data\nimport models\nimport utils as vae_utils\nfrom flax.training import train_state, checkpoints\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport ml_collections\nimport optax\nimport tensorflow_datasets as tfds\n\n# @jax.vmap\n# def binary_cross_entropy_with_logits(logits, labels):\n#   logits = nn.log_sigmoid(logits)\n#   return -jnp.sum(\n#       labels * logits + (1.0 - labels) * jnp.log(-jnp.expm1(logits))\n#   )\n\n\ndef compute_metrics(recon_x, x):\n    # loss = binary_cross_entropy_with_logits(recon_x, x).mean()\n    loss = jnp.mean(jnp.square(recon_x - x))\n    return {'loss': loss }\n\n\ndef train_step(state, batch, z_rng, latents):\n    def loss_fn(params):\n        recon_x = models.model(latents).apply(\n            {'params': params}, batch, z_rng\n        )\n\n        loss = jnp.mean(jnp.square(recon_x - batch))\n        # loss = binary_cross_entropy_with_logits(recon_x, batch).mean()\n        return loss\n\n    grads = jax.grad(loss_fn)(state.params)\n    return state.apply_gradients(grads=grads)\n\n\ndef eval_f(params, images, z, z_rng, latents):\n    def eval_model(ae):\n        recon_images = ae(images, z_rng)\n        comparison = jnp.concatenate([\n            images[:8].reshape(-1, 28, 28, 1),\n            recon_images[:8].reshape(-1, 28, 28, 1),\n        ])\n\n        generate_images = ae.generate(z)\n        generate_images = generate_images.reshape(-1, 28, 28, 1)\n        metrics = compute_metrics(recon_images, images)\n        return metrics, comparison, generate_images\n\n    return nn.apply(eval_model, models.model(latents))({'params': params})\n\n\ndef train_and_evaluate(config: ml_collections.ConfigDict):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    print('Initializing dataset.')\n    idxs = nsd_data.shuffle_idxs()\n    train_loader, test_loader = nsd_data.create_loaders(idxs, roi=None, batch_size=config.batch_size)\n    train_ds = iter(train_loader)\n    test_ds = iter(test_loader)\n\n    print('Initializing model.')\n    init_data = jnp.ones((config.batch_size, 4, 2000), jnp.float32)  # fmri of 4 shown-images, 2000 voxels each\n\n    print('initialising params')\n    params = models.model(config.latents).init(key, init_data, rng)['params']\n\n    print('Initializing state')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latents).apply,\n        params=params,\n        tx=optax.adam(config.learning_rate),\n    )\n\n    rng, z_key, eval_rng = random.split(rng, 3)\n    z = random.normal(z_key, (64, config.latents))\n\n        # ds_builder.info.splits['train'].num_examples\n    number_images = 600\n    steps_per_epoch = (number_images // int(config.batch_size)) # n examples)\n\n    print('starting training')\n    for epoch in range(config.num_epochs):\n        for s in range(steps_per_epoch):\n            # images, lh_fmri, rh_fmri = next(train_ds)\n            batch = next(train_ds)\n            rng, key = random.split(rng)\n            state = train_step(state, batch, key, config.latents)\n\n            # metrics, comparison, sample = eval_f(\n            #     state.params, test_ds, z, eval_rng, config.latents\n            # )\n            # vae_utils.save_image(\n            #     comparison, f'results/reconstruction_{epoch}.png', nrow=8\n            # )\n            # vae_utils.save_image(sample, f'results/sample_{epoch}.png', nrow=8)\n\n            # print(\n            #     'eval epoch: {}, loss: {:.4f}'.format(\n            #         epoch + 1, metrics['loss']\n            #     )\n            # )\n\n            \nthis was the updated code. now i get `   batch = next(train_ds)\n         ^^^^^^^^^^^^^^\nStopIteration`\n\nThe StopIteration error occurs when you've reached the end of your iterator (train_ds). To fix this, you need to reinitialize your data loader at the start of each epoch. Here's the corrected version:\n\n```python\ndef train_and_evaluate(config: ml_collections.ConfigDict):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    print('Initializing dataset.')\n    idxs = nsd_data.shuffle_idxs()\n    train_loader, test_loader = nsd_data.create_loaders(idxs, roi=None, batch_size=config.batch_size)\n\n    print('Initializing model.')\n    init_data = jnp.ones((config.batch_size, 4, 2000), jnp.float32)  # fmri of 4 shown-images, 2000 voxels each\n\n    print('initialising params')\n    params = models.model(config.latents).init(key, init_data, rng)['params']\n\n    print('Initializing state')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latents).apply,\n        params=params,\n        tx=optax.adam(config.learning_rate),\n    )\n\n    rng, z_key, eval_rng = random.split(rng, 3)\n    z = random.normal(z_key, (64, config.latents))\n\n    number_images = 600\n    steps_per_epoch = (number_images // int(config.batch_size))\n\n    print('starting training')\n    for epoch in range(config.num_epochs):\n        # Reinitialize data loaders at the start of each epoch\n        train_ds = iter(train_loader)\n        test_ds = iter(test_loader)\n        \n        for s in range(steps_per_epoch):\n            try:\n                batch = next(train_ds)\n                rng, key = random.split(rng)\n                state = train_step(state, batch, key, config.latents)\n            except StopIteration:\n                print(f\"Reached end of dataset in epoch {epoch}\")\n                break\n```\n\nThe key changes are:\n\n1. Moved the iterator creation (`iter(train_loader)`) inside the epoch loop so it's reset for each epoch\n2. Added a try-except block to handle StopIteration gracefully\n3. Kept the train_loader and test_loader creation outside the epoch loop as they don't need to be recreated\n\nThis should prevent the StopIteration error and allow your training to continue properly across epochs.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":97}}},{"id":{"replica_id":0,"value":10},"start":3900,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":98}}},{"id":{"replica_id":0,"value":11},"start":5173,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":99}}},{"id":{"replica_id":0,"value":48},"start":8820,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":103}}},{"id":{"replica_id":0,"value":83},"start":8924,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":104}}},{"id":{"replica_id":0,"value":84},"start":11077,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":102}}}],"summary":"State Params NoneType Error Fix","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}