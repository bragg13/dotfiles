{"id":"17d5ed55-2e25-4802-b088-3b8ee0b1a0fb","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs/simple_autoencoder/train.py:1-196\n# Excerpt from: \n\"\"\"Training and evaluation logic.\"\"\"\nimport matplotlib.pyplot as plt\nfrom flax import linen as nn\nimport models\nfrom logger import log\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport optax\nfrom tqdm import tqdm\nfrom nsd_data import get_train_test_datasets, get_batches\nfrom visualisations import plot_losses\nfrom flax.training import train_state\n\ndef compute_metrics(recon_x, x):\n    loss = jnp.mean(jnp.square(recon_x - x))\n    return {'loss': loss }\n\ndef train_step(state, batch, key, latent_dim, l1_coefficient=0.01):\n    \"\"\"Performs a single training step updating model parameters based on batch data.\n\n    Args:\n        state: TrainState object containing model state and optimizer\n        batch: Current batch of training data\n        key: Random key for generating latent vectors\n        latent_dim: Dimension of the latent space\n        l1_coefficient: Optional coefficient for L1 regularization (default 0.01)\n\n    Returns:\n        Tuple containing updated state and training loss for the batch\n    \"\"\"\n    def loss_fn(params, key):\n        fmri_voxels = batch.shape[1]\n        recon_x, latent_vec = models.model(latent_dim, fmri_voxels).apply(\n            {'params': params}, batch, key\n        )\n        mse_loss = jnp.mean(jnp.square(recon_x - batch))\n        # sparsity_loss = l1_coefficient * jnp.sum(latent_vec) # jnp.sum(jnp.abs(latent_vec))\n\n        return mse_loss\n\n    loss, grads = jax.value_and_grad(loss_fn)(state.params, key)\n    return state.apply_gradients(grads=grads), loss\n\ndef evaluate_fun(params, loader, key, latent_dim, num_steps=2):\n    \"\"\"Evaluation function that computes metrics on batches.\n       Evaluates 2 batches by default (during training),\n       while evaluating the entire dataset during testing.\n\n    Args:\n        params: Model parameters to evaluate\n        train_loader: Generator yielding training batches\n        key: Random key for generating latent vectors\n        latent_dim: Dimension of the latent space\n        num_steps: Number of evaluation steps to perform\n\n    Returns:\n        List of tuples containing (metrics, data, latent vectors) for each evaluation step\n        Average of the losses\n    \"\"\"\n    def eval_model(ae):\n        reconstruction, latent_vecs = ae(batch, key)\n        metrics = compute_metrics(reconstruction, batch)\n        return metrics, (batch, reconstruction), latent_vecs\n\n    evaluations = []\n    for _ in range(num_steps):\n        batch = next(loader)\n        fmri_voxels = batch.shape[1]\n        eval = nn.apply(eval_model, models.model(latent_dim, fmri_voxels))({'params': params})\n        evaluations.append(eval)\n    return evaluations\n\ndef train_and_evaluate(config):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, key = random.split(rng)\n\n    log('Initializing dataset...', 'TRAIN')\n    train_ds, test_ds = get_train_test_datasets(subject=3, roi_class='floc-bodies', hem='lh')\n    print(f\"training ds shape: {train_ds.shape}\")\n    print(f\"test ds shape: {test_ds.shape}\")\n    train_loader = get_batches(train_ds, config.batch_size)\n    test_loader = get_batches(test_ds, config.batch_size)\n    train_size = train_ds.shape[0]\n    test_size = test_ds.shape[0]\n    fmri_voxels = train_ds.shape[1]\n\n\n    log('Initializing model...', 'TRAIN')\n    # va bene che siano uni o random gaussian noise?\n    init_data = jnp.ones((config.batch_size, fmri_voxels), jnp.float32)  # fmri of 4 shown-images, 2000 voxels each\n\n    log('Initializing params...', 'TRAIN')\n    params = models.model(config.latent_dim, fmri_voxels).init(key, init_data, rng)['params']\n\n    log('Initializing state...', 'TRAIN')\n    state = train_state.TrainState.create(\n        apply_fn=models.model(config.latent_dim, fmri_voxels).apply,\n        params=params,\n        tx=optax.adamw(config.learning_rate),\n    )\n\n    rng, z_key, eval_rng = random.split(rng, 3)\n\n    log(f'Calculating training steps per epochs (train_size: {train_size})...', 'TRAIN')\n    steps_per_epoch = train_size // int(config.batch_size)\n    if train_size % int(config.batch_size) != 0:\n        steps_per_epoch += 1\n    log(f\"{steps_per_epoch} steps for each ({config.num_epochs}) epoch\", 'TRAIN')\n\n\n    log('\\nstarting training', 'TRAIN')\n    train_losses = []\n    eval_losses = []\n    test_losses = []\n\n    for epoch in range(config.num_epochs):\n        # log(f'Epoch {epoch + 1}/{config.num_epochs}', 'TRAIN LOOP')\n        rng, epoch_key = jax.random.split(rng)\n        epoch_train_losses = []\n        epoch_eval_losses = []\n\n        # Training loop\n        for step, batch in (pbar := tqdm(enumerate(train_loader), total=steps_per_epoch)):\n            if step >= steps_per_epoch:\n                break\n\n            state, loss = train_step(state, batch, epoch_key, config.latent_dim)\n            epoch_train_losses.append(loss)\n\n            # every 25% of steps for this epoch, evaluate the model\n            if step % (steps_per_epoch // 4) == 0:\n                rng, key = jax.random.split(rng)\n                evaluations = evaluate_fun(\n                    state.params, train_loader, key, config.latent_dim, num_steps=5\n                )\n\n                # calculate average loss among evaluations\n                eval_loss = sum([eval[0]['loss'] for eval in evaluations]) / len(evaluations)\n                epoch_eval_losses.append(eval_loss)\n\n                # only plotting the first evaluation\n                metrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[0]\n                fig,axs = plt.subplots(5, 3, figsize=(15,15))\n                fig.suptitle('Validation set', fontsize=14)\n                axs[0, 0].set_title('original')\n                axs[0, 1].set_title('reconstructed')\n                axs[0, 2].set_title('difference')\n\n                # plot the first 5 samples of the first batch evaluated\n                for i in range(5):\n                    original = evaluated_batches[i][:3600].reshape(36, 100)\n                    reconstruction = reconstructions[i][:3600].reshape(36, 100)\n                    axs[i, 0].imshow(original, cmap='gray')\n                    axs[i, 1].imshow(reconstruction, cmap='gray')\n                    axs[i, 2].imshow(original - reconstruction, cmap='gray')\n\n                # should I append the eval loss to the training losses?\n                # train_losses.append(metrics['loss'])\n                fig.savefig(f'./results/epoch_{epoch}_step_{step}.png')\n                plt.close()\n                plt.close(fig)\n\n            pbar.set_description(f\"step {step} loss: {str(loss)[:5]}\")\n        train_losses.append(sum(epoch_train_losses)/len(epoch_train_losses))\n        eval_losses.append(sum(epoch_eval_losses)/len(epoch_eval_losses))\n\n\n\n    # evaluate on the test set\n    rng, key = jax.random.split(rng)\n    evaluations = evaluate_fun(\n        state.params, test_loader, key, config.latent_dim, num_steps=test_size // config.batch_size\n    )\n    metrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[0]\n    test_losses = [eval[0]['loss'] for eval in evaluations]\n\n    # plotting only the first 5 samples of the first batch evaluated\n    fig, axs = plt.subplots(5, 3, figsize=(15,15))\n    fig.suptitle('Testing set', fontsize=14)\n    axs[0, 0].set_title('original')\n    axs[0, 1].set_title('reconstructed')\n    axs[0, 2].set_title('difference')\n    for i in range(5):\n        original = evaluated_batches[i][:3600].reshape(36, 100)\n        reconstruction = reconstructions[i][:3600].reshape(36, 100)\n        axs[i, 0].imshow(original, cmap='gray')\n        axs[i, 1].imshow(reconstruction, cmap='gray')\n        axs[i, 2].imshow(original - reconstruction, cmap='gray')\n\n    fig.savefig('./results/testing.png')\n    plt.close()\n\n    plot_losses(train_losses, eval_losses, test_losses)\n\n```\nwhy am i not closing the figures? \nIn your code, you are closing the figures, but you're actually calling `plt.close()` twice in succession during the training loop:\n\n```python\nfig.savefig(f'./results/epoch_{epoch}_step_{step}.png')\nplt.close()\nplt.close(fig)  # Redundant close\n```\n\nYou only need one call to close the figure. You can either use:\n```python\nplt.close(fig)  # Close specific figure\n```\nOR\n```python\nplt.close()  # Close current figure\n```\n\nBoth will work, but using `plt.close(fig)` is more explicit and safer as it ensures you're closing the specific figure you created.\n\nThe redundant closing isn't causing any problems, but it's unnecessary. You should update your code to use just one close statement for better clarity and efficiency.\n\nFor the final test set visualization, you have the correct single close:\n```python\nfig.savefig('./results/testing.png')\nplt.close()\n```\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":27}}},{"id":{"replica_id":0,"value":18},"start":7864,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":28}}},{"id":{"replica_id":0,"value":19},"start":8722,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":29}}}],"summary":"Closing Matplotlib Figure Memory Usage","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}