{"id":"1c10f96d-4b18-444d-8e14-b391361104f3","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs_code/src/model/data/nsd_data.py:1-354\n# Excerpt from: \nimport os\nimport numpy as np\nimport jax.numpy as jnp\nimport pandas as pd\nfrom jax import random\nfrom sklearn.model_selection import train_test_split\nfrom data.roi import load_roi_data\nfrom typing import Literal\nimport logging\nfrom data.coco_load import CocoLoader\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass NSDDataLoader:\n    def __init__(self, data_dir, subject=3, roi_class='floc-bodies', hem='lh'):\n        self.subject = subject\n        self.data_dir = f\"{ data_dir }/nsd_data\"\n        self.roi_class = roi_class\n        self.hem = hem\n        self.coco_loader = CocoLoader(data_dir)\n        self.images_to_nsd = self.get_images_to_nsd_df()\n\n\n    def get_images_to_nsd_df(self):\n        \"\"\"\n            Load the image list indices for a given subject.\n\n            Returns:\n                pd.DataFrame: DataFrame containing image list indices and corresponding nsd indices\n        \"\"\"\n        path_img2nsd = os.path.join(self.data_dir, f\"subj0{self.subject}\", \"training_split\", 'images_to_nsd.csv')\n        if os.path.exists(path_img2nsd):\n            logger.info(f'Found file **images_to_csd.csv* for subject. Loading image list indices from {path_img2nsd}')\n            return pd.read_csv(path_img2nsd)\n        else:\n            logger.info('File **images_to_csd.csv* not foudn for subject. Building image list indices...')\n            images_path = os.path.join(self.data_dir, f\"subj0{self.subject}\", \"training_split\", \"training_images\")\n            images = sorted(os.listdir(images_path))\n            images_to_nsd= {}\n            for i, filename in enumerate(images):\n                start_i = filename.find(\"nsd-\") + len(\"nds-\")\n                nsd_index = int(filename[start_i : start_i + 5])\n                images_to_nsd[i] = [i, nsd_index]\n            logger.info(f'Loaded {len(images_to_nsd)} images for subject {self.subject}')\n\n            df = pd.DataFrame.from_dict(images_to_nsd, orient=\"index\", columns=[\"listIdx\", \"nsdId\"])\n            df.to_csv(path_img2nsd, index=False)\n            return df\n\n    def get_shared_indices(self, category='person'):\n        \"\"\"\n            Get the image indices for shared images with a specified category.\n\n            Args:\n                category (str, optional): The coco category. Defaults to 'person'.\n\n            Returns:\n                tuple: Two numpy arrays containing indices for shared images with and without the specified category respectively\n        \"\"\"\n        shared_df = self.coco_loader.get_shared_df().merge(self.images_to_nsd, on='nsdId')\n        shared_category, shared_not_category = self.coco_loader.split_by_category(shared_df, category)\n        logger.info(f'Found {len(shared_category)} shared images with category {category}')\n        return shared_category[\"listIdx\"].values, shared_not_category[\"listIdx\"].values\n\n    def get_train_test_indices(self):\n        \"\"\"\n            Get the image indices for training and testing sets for a given subject.\n\n            Returns:\n                tuple: Two numpy arrays containing train and test indices respectively:\n                    - train_idxs (np.ndarray): Indices for training set (90% of data)\n                    - test_idxs (np.ndarray): Indices for test set (10% of data)\n        \"\"\"\n        subject_images = pd.merge(self.images_to_nsd, self.coco_loader.get_subject_df(self.subject), on=\"nsdId\", how=\"inner\")\n        train_idxs, test_idxs = train_test_split(np.arange(len(subject_images)), test_size=0.1, random_state=42)\n        logger.info(f'Split data into {len(train_idxs)} training and {len(test_idxs)} testing indices')\n        return train_idxs, test_idxs\n\n\n    def z_score(self, data):\n        return jnp.array((data - data.mean()) / data.std())\n\n    def get_split_masked_datasets(self, indices):\n        \"\"\"\n            Get training and test fMRI datasets for a specified subject and ROI class; mask the data by ROI.\n\n            Returns:\n                tuple: Two arrays containing train and test fMRI data respectively.\n                For hem='all', arrays contain concatenated data from both hemispheres.\n                For hem='lh'/'rh', arrays contain data from specified hemisphere only.\n        \"\"\"\n        # get the paths to the fmri data\n        fmri_base_path = os.path.join(self.data_dir, f\"subj0{self.subject}\", \"training_split\", \"training_fmri\")\n        lh_fmri_path = os.path.join(fmri_base_path, \"lh_training_fmri.npy\")\n        rh_fmri_path = os.path.join(fmri_base_path, \"rh_training_fmri.npy\")\n\n        # get the indices of the training and testing sets, or shared images\n        first_idxs, sec_idxs = indices\n        first_lh_fmri = jnp.load(lh_fmri_path)[first_idxs]\n        first_rh_fmri = jnp.load(rh_fmri_path)[first_idxs]\n        sec_lh_fmri = jnp.load(lh_fmri_path)[sec_idxs]\n        sec_rh_fmri = jnp.load(rh_fmri_path)[sec_idxs]\n\n        # get the ROI mask\n        roi_data = load_roi_data(self.subject)\n        roi_lh, roi_rh = roi_data['challenge']['lh'][self.roi_class] > 0, roi_data['challenge']['rh'][self.roi_class] > 0\n\n        # mask the data by ROI\n        first_lh_fmri = first_lh_fmri[:, roi_lh]\n        first_rh_fmri = first_rh_fmri[:, roi_rh]\n        sec_lh_fmri = sec_lh_fmri[:, roi_lh]\n        sec_rh_fmri = sec_rh_fmri[:, roi_rh]\n\n        if self.hem == 'all':\n            train_all_fmri = np.concatenate([first_lh_fmri, first_rh_fmri], axis=1)\n            test_all_fmri = np.concatenate([sec_lh_fmri, sec_rh_fmri], axis=1)\n            logger.info(f'Loaded all hemisphere data with shape {train_all_fmri.shape}')\n            return self.z_score(train_all_fmri), self.z_score(test_all_fmri)\n        elif self.hem == 'lh':\n            logger.info(f'Loaded left hemisphere data with shape {first_lh_fmri.shape}')\n            return self.z_score(first_lh_fmri), self.z_score(sec_lh_fmri)\n        elif self.hem == 'rh':\n            logger.info(f'Loaded right hemisphere data with shape {first_rh_fmri.shape}')\n            return self.z_score(first_rh_fmri), self.z_score(sec_rh_fmri)\n        else:\n            raise ValueError(f\"Invalid hemisphere selection: {self.hem}. Must be 'all', 'lh', or 'rh'.\")\n\n    def get_train_test_datasets(self):\n        indices = self.get_train_test_indices()\n        return self.get_split_masked_datasets(indices)\n\n    def get_analysis_datasets(self, category):\n        indices = self.get_shared_indices(category)\n        return self.get_split_masked_datasets(indices)\n\n    def get_batches(self, fmri, key, batch_size: int):\n        \"\"\"Create batches of fMRI data with the specified batch size.\n\n        Args:\n            fmri: Array containing fMRI data to be batched\n            batch_size (int): Size of each batch\n\n        Yields:\n            ndarray: Batch of fMRI data with shape (batch_size, voxels)\n        \"\"\"\n\n        num_samples = fmri.shape[0]\n        permutation = random.permutation(key, num_samples // batch_size * batch_size)\n        logger.info(f\"permutatin first: {permutation[:5]}\")\n        return fmri[permutation]\n\n#\n# track image list indices in dataframe\n# def images_to_nsd_df(subject=3):\n#     # training and test images list, sorted\n#     images_path = os.path.join(\"../data\", \"subj0\"+str(subject), \"training_split\", \"training_images\")\n#     images = sorted(os.listdir(images_path))\n\n#     # make a dataframe with mapping image-nsd_index\n#     images_to_nsd= {}\n#     for i, filename in enumerate(images):\n#         start_i = filename.find(\"nsd-\") + len(\"nds-\")\n#         nsd_index = int(filename[start_i : start_i + 5])\n#         images_to_nsd[i] = [i, nsd_index]\n#     images_to_nsd = pd.DataFrame.from_dict(\n#         images_to_nsd, orient=\"index\", columns=[\"listIdx\", \"nsdId\"] # we need listIdx for the shared indices\n\n#     )\n#     # log(f\"total images for subject {subject}: {len(images_to_nsd)}\", 'DATA')\n#     return images_to_nsd\n\n# # get indices for the training, test and analysis split\n# def get_shared_indices(category: str = 'person', subject: int = 3):\n#     images_to_nsd = images_to_nsd_df(subject=subject)\n#     coco_loaded = cl.nsd_coco\n#     shared_df = cl.getSharedDf(coco_loaded).merge(images_to_nsd, on='nsdId')\n#     shared_category, shared_not_category = cl.splitByCategory(shared_df, category)\n#     # category and not indices\n#     # print(shared_category[:5])\n#     # print(shared_not_category[:5])\n#     category_idxs = shared_category[\"listIdx\"].values\n#     not_category_idxs = shared_not_category[\"listIdx\"].values\n#     return category_idxs, not_category_idxs\n\n# def get_train_test_indices(subject=3):\n#     \"\"\"\n#     Get the image indices for training and testing sets for a given subject.\n\n#     Args:\n#         subject (int, optional): Subject ID number (1-8). Defaults to 3.\n\n#     Returns:\n#         tuple: Two numpy arrays containing train and test indices respectively:\n#             - train_idxs (np.ndarray): Indices for training set (90% of data)\n#             - test_idxs (np.ndarray): Indices for test set (10% of data)\n#     \"\"\"\n\n#     # map coco categories to the pics in the dataset\n#     images_to_nsd = images_to_nsd_df(subject)\n#     coco_loaded = cl.nsd_coco\n#     subject_coco_df = cl.getSubjDf(coco_loaded, subject)\n#     subject_images = pd.merge(images_to_nsd, subject_coco_df, left_on=\"nsdId\", right_on=\"nsdId\", how=\"inner\")\n\n#     train_idxs, test_idxs = train_test_split(np.arange(len(subject_images)), test_size=0.1, random_state=42)\n#     return train_idxs, test_idxs\n\n# #  normalise data\n# # def normalise(_max, _min, data):\n# #    return jnp.array((data - _min) / (_max - _min))\n# def z_score(data):\n#     normalised = jnp.array((data - data.mean()) / data.std())\n#     return normalised\n\n# #  get dataset splits based on indices\n# def get_split_masked_datasets(indices, subject, roi_class='floc-bodies', hem: Literal[\"all\", \"lh\", \"rh\"]='lh') -> tuple:\n#     \"\"\"Get training and test fMRI datasets for a specified subject and ROI class.\n\n#     Args:\n#         subject (int, optional): The subject ID number (1-8). Defaults to 3.\n#         roi_class (str, optional): Region of interest class name. Defaults to 'floc-bodies'.\n#         hem (str, optional): Hemisphere selection ('all', 'lh', or 'rh'). Defaults to 'all'.\n\n#     Returns:\n#         tuple: Two arrays containing train and test fMRI data respectively:\n#             - train_fmri: Training fMRI data array\n#             - test_fmri: Test fMRI data array\n#             For hem='all', arrays contain concatenated data from both hemispheres.\n#             For hem='lh'/'rh', arrays contain data from specified hemisphere only.\n#     \"\"\"\n#     log('creating datasets...', 'DATA')\n#     # get the paths to the fmri data\n#     fmri_base_path = os.path.join(\"../data\", \"subj0\"+str(subject), \"training_split\", \"training_fmri\")\n#     lh_fmri_path = os.path.join(fmri_base_path, \"lh_training_fmri.npy\")\n#     rh_fmri_path = os.path.join(fmri_base_path, \"rh_training_fmri.npy\")\n\n#     # get the indices for training and testing\n#     first_idxs, sec_idxs = indices\n\n#     # load the fmri data, sliced by indexes\n#     # ndr: for one image, there is both the left and right hemisphere (I mean not necessarily, but yea)\n#     # ndr pt2: first and sec is kind of saying train and test, but it's not really that\n#     first_lh_fmri = jnp.load(lh_fmri_path)[first_idxs]\n#     first_rh_fmri = jnp.load(rh_fmri_path)[first_idxs]\n\n#     sec_lh_fmri = jnp.load(lh_fmri_path)[sec_idxs]\n#     sec_rh_fmri = jnp.load(rh_fmri_path)[sec_idxs]\n\n#     # get the ROI mask\n#     roi_data = load_roi_data(subject)\n#     # maske the data by ROI\n#     roi_lh, roi_rh = roi_data['challenge']['lh'][roi_class] > 0, roi_data['challenge']['rh'][roi_class] > 0\n\n#     first_lh_fmri = first_lh_fmri[:, roi_lh]\n#     first_rh_fmri = first_rh_fmri[:, roi_rh]\n#     sec_lh_fmri = sec_lh_fmri[:, roi_lh]\n#     sec_rh_fmri = sec_rh_fmri[:, roi_rh]\n\n#     if hem == 'all':\n#         train_all_fmri = np.concatenate([first_lh_fmri, first_rh_fmri], axis=1)\n#         test_all_fmri = np.concatenate([sec_lh_fmri, sec_rh_fmri], axis=1)\n#         return z_score(train_all_fmri), z_score(test_all_fmri)\n\n#     elif hem == 'lh':\n#         return z_score(first_lh_fmri), z_score(sec_lh_fmri)\n\n#     elif hem == 'rh':\n#         return z_score(first_rh_fmri), z_score(sec_rh_fmri)\n\n#     else:\n#         raise ValueError(f\"Invalid hemisphere selection: {hem}. Must be 'all', 'lh', or 'rh'.\")\n\n# def get_train_test_datasets(subject, roi_class='floc-bodies', hem: Literal[\"all\", \"lh\", \"rh\"]='lh') -> tuple:\n#     \"\"\"\n#     Args:\n#         subject (int, optional): The subject ID number (1-8). Defaults to 3.\n#         roi_class (str, optional): Region of interest class name. Defaults to 'floc-bodies'.\n#         hem (str, optional): Hemisphere selection ('all', 'lh', or 'rh'). Defaults to 'all'.\n#     Returns:\n#         tuple: Two arrays containing train and test fMRI data respectively.\n#     \"\"\"\n#     indices = get_train_test_indices(subject)\n#     return get_split_masked_datasets(indices, subject, roi_class, hem)\n\n# # aka get shared-images fmri data?\n# def get_analysis_datasets(category, subject, roi_class='floc-bodies', hem: Literal[\"all\", \"lh\", \"rh\"]='lh') -> tuple:\n#     \"\"\"\n#     Args:\n#         category (str, optional): The coco category. # Defaults to 'person'.\n#         subject (int, optional): The subject ID number (1-8). Defaults to 3.\n#         roi_class (str, optional): Region of interest class name. Defaults to 'floc-bodies'.\n#         hem (str, optional): Hemisphere selection ('all', 'lh', or 'rh'). Defaults to 'all'.\n#     Returns:\n#         tuple: Two arrays containing fMRI data resulted from shared images of category and not-category respectively.\n#     \"\"\"\n#     indices = get_shared_indices(category, subject) # category and not category indices\n#     return get_split_masked_datasets(indices, subject, roi_class, hem)\n\n# #\n# def get_train_test_mnist():\n#     # load the mnist dataset from tfds\n#     mnist = tfds.load(\"mnist\", split='train')\n#     x_data = jnp.array([x[\"image\"] for x in tfds.as_numpy(mnist)]).reshape(-1, 28*28)\n#     x_data = x_data / 255.0\n#     print(f\"mnist shape: {x_data.shape}\")\n#     train, test = train_test_split(np.arange(len(x_data)), test_size=0.2, random_state=42)\n#     print(f\"train_ds length: {len(train)}, test_ds {len(test)}\")\n#     return x_data[train], x_data[test]\n\n# def get_train_test_cifar10():\n#     def rgb2gray(rgb):\n#         return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\n#     # load the mnist dataset from tfds\n#     cifar = tfds.load(\"cifar10\", split='train')\n#     x_data = jnp.array([rgb2gray(x[\"image\"]) for x in tfds.as_numpy(cifar)]).reshape(-1, 32*32)\n#     x_data = x_data / 255.0\n#     print(f\"cifar shape: {x_data.shape}\")\n#     train, test = train_test_split(np.arange(len(x_data)), test_size=0.2, random_state=42)\n#     print(f\"train_ds length: {len(train)}, test_ds {len(test)}\")\n#     return x_data[train], x_data[test]\n\n\n# #  split data into lh and rh\n# def split_hemispheres(fmri, split_idx=19004):\n#     \"\"\"\n#     Args:\n#         split_idx (int): length of the subject's lh challenge space. Defaults to 19004 (true for most subjects).\n#     \"\"\"\n#     return fmri[:, :split_idx], fmri[:, split_idx:]\n\n# #  unmask data to original challenge space\n# def unmask_from_roi_class(subject, masked, roi_class, hem: Literal[\"all\", \"lh\", \"rh\"], lh_chal_space_size=19004, rh_chal_space_size=20544):\n#     \"\"\"\n#     Args:\n#         lh_chal_space_size (int, optional): length of the subject's lh challenge space. Defaults to 19004 (true for subjects 1,2,3,4,5,7).\n#         rh_chal_space_size (int, optional): length of the subject's rh challenge space. Defaults to 20544 (true for subjects 1,2,3,4,5,7).\n#     \"\"\"\n#     roi_data = load_roi_data(subject)\n#     if hem == 'all':\n#         roi_lh = roi_data['challenge']['lh'][roi_class] > 0\n#         roi_rh = roi_data['challenge']['rh'][roi_class] > 0\n#         roi_mask = np.concatenate([roi_lh, roi_rh], axis=0)\n#         unmasked = np.zeros((masked.shape[0], lh_chal_space_size+rh_chal_space_size))\n#     elif hem == 'lh':\n#         roi_mask = roi_data['challenge']['lh'][roi_class] > 0\n#         unmasked = np.zeros((masked.shape[0], lh_chal_space_size))\n#     elif hem == 'rh':\n#         roi_mask = roi_data['challenge']['rh'][roi_class] > 0\n#         unmasked = np.zeros((masked.shape[0], rh_chal_space_size))\n#     unmasked[:, roi_mask] = masked\n#     return unmasked\n\n```\nin `get_train_test_indices`, why do i get th esubject images, if i just need the len? also, do i really only need the length of that array? cant i just then count how many images are in the folder?\nIn the `get_train_test_indices` method, the process of merging `images_to_nsd` with the subject-specific COCO data (`self.coco_loader.get_subject_df(self.subject)`) ensures that you are working with the correct subset of images that are relevant to the subject. This is important because not all images in the dataset may be used for every subject, and the merge operation filters out only those images that are relevant to the subject.\n\nHowever, if you are certain that all images in the folder are relevant to the subject and there is no need to filter them further, you could simplify the process by directly counting the number of images in the folder. This would avoid the overhead of loading and merging dataframes. Here is how you could modify the `get_train_test_indices` method to achieve this:\n\n```python\ndef get_train_test_indices(self):\n    \"\"\"\n    Get the image indices for training and testing sets for a given subject.\n\n    Returns:\n        tuple: Two numpy arrays containing train and test indices respectively:\n            - train_idxs (np.ndarray): Indices for training set (90% of data)\n            - test_idxs (np.ndarray): Indices for test set (10% of data)\n    \"\"\"\n    images_path = os.path.join(self.data_dir, f\"subj0{self.subject}\", \"training_split\", \"training_images\")\n    num_images = len(os.listdir(images_path))\n    indices = np.arange(num_images)\n    train_idxs, test_idxs = train_test_split(indices, test_size=0.1, random_state=42)\n    logger.info(f'Split data into {len(train_idxs)} training and {len(test_idxs)} testing indices')\n    return train_idxs, test_idxs\n```\n\nThis approach directly counts the number of images in the folder and uses that count to generate the indices for the train-test split. This method is more efficient if you are confident that all images in the folder are relevant to the subject and there is no need for further filtering.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":122}}},{"id":{"replica_id":0,"value":104},"start":16678,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":123}}},{"id":{"replica_id":0,"value":105},"start":18566,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":124}}}],"summary":"Optimizing NSD DataLoader Code","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}