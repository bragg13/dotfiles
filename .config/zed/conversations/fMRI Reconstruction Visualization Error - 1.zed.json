{"id":"808646e6-4f93-4b81-9f9b-5a12605e3d52","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs/simple_autoencoder/surf_plot.py:1-192\n# Excerpt from: \n# %% imports in use\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom nilearn import datasets\nfrom nilearn import plotting\nfrom nilearn import image\nimport nibabel as nib\nfrom roi import ROI_TO_CLASS, CLASS_TO_ROI, load_roi_data\nfrom logger import log\n\n# %% download the dataset, this is also in nsd_data\ndata_dir = '../data'\nsubjects = [1,2,3,4,5,6,7,8]\nimg = 0\nFSAVERAGE = datasets.fetch_surf_fsaverage('fsaverage')\n# the class to load and store data\nclass argObj:\n  def __init__(self, data_dir, subj):\n    self.subj = format(subj, '02')\n    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n\nSUBJECTS = dict()\nfor subject in subjects:\n    args = argObj(data_dir, subject)\n    SUBJECTS[subject] = dict()\n    # load the dataset\n    fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n    SUBJECTS[subject]['lh_fmri'] = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n    SUBJECTS[subject]['rh_fmri'] = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n\n    train_img_dir = os.path.join(args.data_dir, 'training_split', 'training_images')\n    SUBJECTS[subject]['img_dir'] = train_img_dir\n    SUBJECTS[subject]['img_list'] = sorted(os.listdir(train_img_dir))\n\nROI_DATA = {subject: load_roi_data(subject) for subject in subjects}\n\n# %% img plot\ndef plot_img(subj: int, img: int, fig=None):\n    # Load the image\n    img_dir = os.path.join(SUBJECTS[subj]['img_dir'] , SUBJECTS[subj]['img_list'][img])\n    train_img = Image.open(img_dir).convert('RGB')\n\n    # Plot the image\n    if fig is None:\n        plt.figure()\n        plt.axis('off')\n        plt.imshow(train_img)\n        plt.title('Training image: ' + str(img+1))\n        plt.show()\n    else:\n        ax = fig.subplots(1, 1)\n        ax.imshow(train_img)\n        ax.axis('off')\n\n# plot_img(3, 312)\n\n# %% print data for paper figure\ndef print_data(subject, roi_map, challenge_roi_class, fsaverage_roi_class, start=250, end=300):\n    print(f'\\nfloc-bodies: {roi_map}')\n    print(f'fsaverage space: {len(fsaverage_roi_class)}')\n    print(f'\\nlh.floc-bodies_fsaverage_space[{start}:{end}]')\n    print(fsaverage_roi_class[start:end])\n    print(f'challenge space: {len(challenge_roi_class)}')\n    print(f'\\nlh.floc-bodies_challenge_space[{start}:{end}]')\n    print(challenge_roi_class[start:end])\n    print(f'\\nlh_fmri[0][{start}:{end}]')\n    print(f'{SUBJECTS[subject][\"lh_fmri\"][0][start:end]}')\n\n# %% show roi on brain surface map\n\n# not sure where to place this correctly so it doesn't get reloaded everytime\ndef get_roi_data(subj, roi_class, hemi):\n    challenge_roi_class = ROI_DATA[subj]['challenge'][hemi][roi_class] # roi indices mapped to challange space\n    fsaverage_roi_class = ROI_DATA[subj]['fsaverage'][hemi][roi_class] # roi indices mapped to fsaverage\n    roi_map = ROI_DATA[subj]['mapping'][roi_class]['id_to_roi']\n    return challenge_roi_class, fsaverage_roi_class, roi_map\n\n# map challenge space to fsaverage for roi of interest\ndef map_fsaverage_resp(subj, fmri, img, roi, hemisphere: str, full_class=False):\n    challenge_roi_class, fsaverage_roi_class, roi_map = get_roi_data(subj, ROI_TO_CLASS[roi], hemisphere)\n    if full_class:\n        fsvg_roi, ch_roi = fsaverage_roi_class, challenge_roi_class\n    else:\n        # Select the vertices corresponding to the ROI of interest\n        roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)] # the id of the roi\n        challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int) # set all other roi ids to 0 (False)\n        fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n        fsvg_roi, ch_roi = fsaverage_roi, challenge_roi\n\n    fsaverage_response = np.zeros(len(fsvg_roi))\n    fsaverage_response[np.where(fsvg_roi)[0]] = fmri[img, np.where(ch_roi)[0]]\n    return fsaverage_response\n\ndef view_surf(fsaverage_map, hemi, title: str, cmap, vmax=None, vmin=None, sym_cmap=True):\n    \"\"\"\n    Arg title should be ROI or ROI class\n    Args:\n        title (str):\n    \"\"\"\n    if hemi == 'lh': hemi = 'left'\n    elif hemi == 'rh': hemi = 'right'\n    return plotting.view_surf(\n        surf_mesh=FSAVERAGE['flat' + '_' + hemi], # flat_ , pial_ , sphere_\n        surf_map=fsaverage_map,\n        bg_map=FSAVERAGE['sulc_' + hemi],\n        threshold=1e-14,\n        cmap=cmap,\n        colorbar=True,\n        vmax=vmax,\n        vmin=vmin,\n        symmetric_cmap=sym_cmap,\n        title=title + ', ' + hemi + ' hemisphere',\n        darkness=0.2\n        )\n\ndef plot_surf(subj, fsaverage_map, hemi, title: str, cmap, style='flat', fig=None, vmax=None, vmin=None, sym_cmap=True):\n    \"\"\"Arg title should be ROI or ROI class\"\"\"\n    if style == 'flat': view = (90, -90)\n    elif (style == 'infl' or style == 'sphere') and hemi == 'lh': view = (-30, -120)\n    elif (style == 'infl' or style == 'sphere') and hemi == 'rh': view = (-30, -60)\n    if hemi == 'lh': hemi = 'left'\n    elif hemi == 'rh': hemi = 'right'\n    return plotting.plot_surf(\n        surf_mesh=FSAVERAGE[style + '_' + hemi], # infl_, flat_ , pial_ , sphere_\n        surf_map=fsaverage_map,\n        bg_map=FSAVERAGE['area_' + hemi], # sulc_ , curv_\n        threshold=1e-14,\n        cmap=cmap,\n        colorbar=False,\n        darkness=0.2,\n        vmax=vmax,\n        vmin=vmin,\n        symmetric_cmap=sym_cmap,\n        # title= title + ', ' + hemi + ' hemisphere',\n        title_font_size=10,\n        view=view, # {“lateral”, “medial”, “dorsal”, “ventral”, “anterior”, “posterior”} or pair (0, -180.0)\n        hemi=hemi,\n        figure=fig,\n        # output_file=f\"./results/subj{subj}_roi_classes_{hemi}.png\"\n        )\n\n# %% plot and view functions\ndef viewRoiClass(subj, roi_class, hemi, cmap):\n    _, fsaverage_roi_class, _ = get_roi_data(subj, roi_class, hemi)\n    return view_surf(fsaverage_roi_class, hemi, f'subj {subj}, {roi_class}', cmap, vmax=4.,vmin=1.,sym_cmap=False)\n\ndef viewRoiClassValues(subj, fmri, img, roi_class, hemi, cmap):\n    fsaverage_response = map_fsaverage_resp(subj, fmri, img, CLASS_TO_ROI[roi_class][0], hemi, full_class=True)\n    return view_surf(fsaverage_response, hemi, roi_class, cmap)\n\ndef viewRoiValues(subj, fmri, img, roi, hemi, cmap):\n    fsaverage_response = map_fsaverage_resp(subj, fmri, img, roi, hemi)\n    return view_surf(fsaverage_response, hemi, roi, cmap)\n\ndef plotRoiClass(subj, roi_class, hemi, cmap, style='infl', fig=None):\n    _, fsaverage_roi_class, _ = get_roi_data(subj, roi_class, hemi)\n    return plot_surf(subj, fsaverage_roi_class, hemi, f'subj {subj}', cmap, style=style, fig=fig, vmax=4.,vmin=1., sym_cmap=False)\n\ndef plotRoiClassValues(subj, fmri, img, roi_class, hemi, cmap, style, fig=None):\n    fsaverage_response = map_fsaverage_resp(subj, fmri, img, CLASS_TO_ROI[roi_class][0], hemi, full_class=True)\n    return plot_surf(subj, fsaverage_response, hemi, title='', cmap=cmap, style=style, fig=fig)\n\n# %% test view in browser\n# first_test_idx = 2126 # check nsd_data\n# third_test_idx = 4894 # check nsd_data\n# roiclassvalues = viewRoiClassValues(lh_fmri, third_test_idx, 'floc-bodies', hemi='lh', cmap='cold_hot')\n# roiclassvalues.open_in_browser()\n# roiclass = viewRoiClass(1, 'floc-bodies', hemi='lh', cmap='Dark2')\n# roiclass.open_in_browser()\n# roivalues = viewRoiValues(lh_fmri, img, 'EBA', 'lh', cmap='blue_transparent_full_alpha_range')\n# roivalues.open_in_browser()\n\n# plotRoiClass(1,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(1,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(2,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(2,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(3,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(3,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(4,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(4,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(5,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(5,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(6,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(6,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(7,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(7,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(8,'floc-bodies', hemi='lh', cmap='gist_rainbow', style='infl')\n# plotRoiClass(8,'floc-bodies', hemi='rh', cmap='gist_rainbow', style='infl')\n# plotRoiClassValues(rh_fmri, img, 'floc-bodies', hemi='rh', cmap='cold_hot', style='sphere')\n\n```\nthis is my code to plot on the brain surface.\n\n\n```py aigs/simple_autoencoder/visualisations.py:1-253\n# Excerpt from: \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom typing import Tuple, List\nimport jax.numpy as jnp\nfrom surf_plot import plotRoiClassValues, SUBJECTS, plot_img, plotRoiClass\nfrom nsd_data import split_hemispheres, unmask_from_roi_class\n\nds_sizes = {\n    'mnist': (28, 28),\n    'cifar10': (32, 32),\n    'fmri': (100, 32),\n}\n# training related\ndef plot_losses(train_losses_mse: List, train_losses_spa: List, results_folder: str, eval_losses: List, steps_per_epoch: int)-> None:\n    num_steps = len(train_losses_mse)\n    indices_eval = list(range(0, num_steps, num_steps//len(eval_losses)))\n    _min = min(len(indices_eval), len(eval_losses))\n\n    plt.figure(figsize=(10,10))\n    plt.title('Train Losses over steps')\n    plt.plot(train_losses_mse, label='train mse', color='blue')\n    # plt.plot(train_losses_spa, label='train spa', color='gray')\n    plt.plot(eval_losses, label='eval', color='orange')\n    plt.xlabel('steps')\n\n    # add xticks for epochs\n    plt.xticks(np.arange(0, len(train_losses_mse), steps_per_epoch))\n    plt.ylabel('loss')\n    plt.grid()\n    plt.legend()\n    plt.savefig(f'{results_folder}/losses.png')\n    plt.close()\n\n\ndef plot_original_reconstruction(originals: jnp.ndarray, reconstructions: jnp.ndarray, config, epoch: int )-> None:\n    fig,axs = plt.subplots(5, 3, figsize=(15,15))\n    axs[0, 0].set_title('original')\n    axs[0, 1].set_title('reconstructed')\n    axs[0, 2].set_title('difference')\n    # print(originals.shape)\n\n    # plot the first 5 samples of the first batch evaluated\n    h, w = ds_sizes[config.ds]\n    for i in range(5):\n        original = originals[i][:h*w].reshape(h, w) #[:3600].reshape(36, 100)\n        reconstruction = reconstructions[i][:h*w].reshape(h, w)\n        axs[i, 0].imshow(original, cmap='viridis')\n        axs[i, 1].imshow(reconstruction, cmap='viridis')\n        axs[i, 2].imshow(np.floor(original*100)/100 - np.floor(reconstruction*100)/100, cmap='gray')\n\n    fig.savefig(f'{config.results_folder}/reconstruction_{epoch}.png')\n\ndef plot_original_reconstruction_fmri(subject:int, originals, reconstructions, hem, roi_class='floc-bodies', style='infl', cmap='cold_hot', total_surface_size=19004+20544):\n    \"\"\"\n    Args:\n        style (str, optional): ['infl', 'flat', 'sphere']. Defaults to 'infl'.\n        total_surface_size (int, otpional): sum of lh fmri size and rh fmri size of the subject. Defaults to 19004 + 20544 (true for most subjects).\n    \"\"\"\n    originals = unmask_from_roi_class(subject, originals, roi_class, hem, (originals.shape[0], 19004))\n    reconstructions = unmask_from_roi_class(subject, reconstructions, roi_class, hem, (reconstructions.shape[0], 19004))\n\n    originals_lh, originals_rh = split_hemispheres(originals)\n    recon_lh, recon_rh = split_hemispheres(reconstructions)\n\n    fig = plt.figure(layout='constrained', figsize=(16, 12))\n    fig.suptitle(f'Trained Subject {subject}')\n    ogs, recons = fig.subfigures(1, 2, wspace=0.0)\n    ogs.suptitle('original')\n    recons.suptitle('reconstructed')\n\n    def create_figs(fig):\n        lh, rh = fig.subfigures(1, 2, wspace=0.0)\n        lh.suptitle('left hemisphere')\n        rh.suptitle('right hemisphere')\n        return lh.subfigures(3, 1, wspace=0, hspace=0.0), rh.subfigures(3, 1, wspace=0.0, hspace=0.0)\n\n    og_lhs, og_rhs = create_figs(ogs)\n    recon_lhs, recon_rhs = create_figs(recons)\n\n    for i in range(3):\n        plotRoiClassValues(subject, fmri=originals_lh, img=i, roi_class=roi_class, hemi='lh', cmap=cmap, style=style, fig=og_lhs[i])\n        plotRoiClassValues(subject, fmri=originals_rh, img=i, roi_class=roi_class, hemi='rh', cmap=cmap, style=style, fig=og_rhs[i])\n        plotRoiClassValues(subject, fmri=recon_lh, img=i, roi_class=roi_class, hemi='lh', cmap=cmap, style=style, fig=recon_lhs[i])\n        plotRoiClassValues(subject, fmri=recon_rh, img=i, roi_class=roi_class, hemi='rh', cmap=cmap, style=style, fig=recon_rhs[i])\n\n    fig.savefig(f'./results/subj{subject}_ogs_recons.png', bbox_inches='tight', dpi=150)\n\n# %%\ndef plot_img_and_fmris(shared_idxs_all, subjects=[1,2,3,4,5], roi_class='floc-bodies'):\n    \"\"\"Args:\n        shared_idxs_all (dict): is a dictionary mapping subjects to a tuple of the category and non category image indices\n    \"\"\"\n    img_subj=subjects[0]\n    img_cat, img_non = shared_idxs_all[img_subj]\n    count = len(img_cat) + len(img_non)\n    countSub = len(subjects)\n\n    fig = plt.figure(layout='constrained', figsize=(12, 12))\n    left, right = fig.subfigures(1, 2, wspace=0.0, width_ratios=[1,2])\n    left.suptitle('Stimuli')\n    right.suptitle(\"Lh and rh fMRI responses\")\n\n    fmris = right.subfigures(count, 1, wspace=0.0, hspace=0.02)\n    imgs = left.subfigures(count, 1, hspace=0.02)\n\n    def create_figs(fig):\n        subjs = fig.subfigures(2, countSub, wspace=0.0, hspace=0.0)\n        for i in range(countSub): subjs[0, i].suptitle(f\"Subj {subjects[i]}\")\n        return subjs\n    subjs = [create_figs(fmri) for fmri in fmris]\n\n    for i, img in enumerate(img_cat):\n        imgs[i].suptitle(f\"Category: Person\")\n        plot_img(img_subj, img, imgs[i])\n        for j, subId in enumerate(subjects):\n            img = shared_idxs_all[subId][0][i]\n            plotRoiClassValues(subId, SUBJECTS[subId]['lh_fmri'], img, roi_class, 'lh', cmap='cold_hot', style='sphere', fig=subjs[i][0, j])\n            plotRoiClassValues(subId, SUBJECTS[subId]['rh_fmri'], img, roi_class, 'rh', cmap='cold_hot', style='sphere', fig=subjs[i][1, j])\n\n    for i, img in enumerate(img_non):\n        imgs[i+len(img_cat)].suptitle(f\"No Person\")\n        plot_img(img_subj, img, imgs[i+len(img_cat)])\n        for j, subId in enumerate(subjects):\n            img = shared_idxs_all[subId][1][i]\n            plotRoiClassValues(subId, SUBJECTS[subId]['lh_fmri'], img, roi_class, 'lh', cmap='cold_hot', style='sphere', fig=subjs[i+2][0, j])\n            plotRoiClassValues(subId, SUBJECTS[subId]['rh_fmri'], img, roi_class, 'rh', cmap='cold_hot', style='sphere', fig=subjs[i+2][1, j])\n\n    fig.savefig(f'./results/stimuli-and-fmris-2.png', bbox_inches='tight', dpi=150)\n\ndef plot_roi_class_subjs(roi_class: str, subjects: list):\n    fig = plt.figure(layout='constrained', figsize=(16, 12))\n    subj_figs = fig.subfigures(3, 3, wspace=0.0)\n\n    for i, subjId in enumerate(subjects):\n        row = i // 3\n        col = i % 3\n\n        subj_fig = subj_figs[row, col]\n        subj_fig.suptitle(f'Subj {subjId}, lh and rh')\n        lh, rh = subj_fig.subfigures(1,2, wspace=-0.5)\n        plotRoiClass(subjId, roi_class, 'lh', cmap='gist_rainbow', style='infl', fig=lh)\n        plotRoiClass(subjId, roi_class, 'rh', cmap='gist_rainbow', style='infl', fig=rh)\n\n    fig.savefig(f'./results/subjs-{roi_class}-rois.png', bbox_inches='tight', dpi=150)\n\n\n# latent vector related\ndef visualize_latent_activations(latent_vecs: jnp.ndarray,\n                               images: jnp.ndarray,\n                               config,\n                               epoch: int | str,\n                               num_examples: int = 5) -> None:\n    \"\"\"\n    Creates a grid showing images and their corresponding latent activations.\n\n    Args:\n        latent_vecs: Array of latent vectors [batch_size, latent_dim]\n        images: Original input images [batch_size, height, width]\n        num_examples: Number of examples to show\n    \"\"\"\n    fig, axes = plt.subplots(2, num_examples, figsize=(15, 4))\n    plt.suptitle('Samples with their Latent Representations')\n\n    h, w = ds_sizes[config.ds]\n    for i in range(num_examples):\n        # Show original image\n        img = images[i][:h*w].reshape(h, w)\n        # if len(images.shape) == 2:  # If images are flattened\n        #     img_size = int(np.sqrt(images.shape[1]))\n        #     img = images[i][:3600].reshape(img_size, img_size)\n        # else:\n        #     img = images[i][:3600]\n        axes[0, i].imshow(img, cmap='gray')\n        axes[0, i].axis('off')\n\n        # Show latent activations as bar plot\n        axes[1, i].bar(range(len(latent_vecs[i])), latent_vecs[i])\n        axes[1, i].set_ylim([latent_vecs.min(), latent_vecs.max()])\n        axes[1, i].set_title(f'Latent Vector {i+1}')\n\n    plt.tight_layout()\n    plt.savefig(f'{config.results_folder}/latent_activations_{epoch}.png')\n\ndef plot_latent_heatmap(latent_vecs: jnp.ndarray,\n                       images: jnp.ndarray,\n                       config,\n                       epoch: int,\n                       num_examples: int = 10) -> None:\n    \"\"\"\n    Creates a heatmap of latent activations for multiple examples.\n\n    Args:\n        latent_vecs: Array of latent vectors [batch_size, latent_dim]\n        images: Original input images [batch_size, height, width]\n        num_examples: Number of examples to show\n    \"\"\"\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(latent_vecs[:num_examples].T,\n                cmap='RdBu_r',\n                center=0,\n                cbar_kws={'label': 'Activation'})\n    plt.xlabel('Sample')\n    plt.ylabel('Latent Dimension')\n    plt.title('Latent Space Activation Patterns')\n    plt.savefig(f'{config.results_folder}/latent_heatmap_{epoch}.png')\n\ndef track_latent_statistics(latent_vecs) -> Tuple[List[float], List[float]]:\n    \"\"\"\n    Compute statistics about latent vector sparsity and activation.\n\n    Args:\n        latent_vecs: Array of latent vectors [batch_size, latent_dim]\n\n    Returns:\n        sparsity: Percentage of zero/near-zero activations\n        mean_activation: Mean absolute activation value\n    \"\"\"\n    threshold = 1e-5  # Threshold for considering a value as \"zero\"\n    sparsity = (np.abs(latent_vecs) < threshold).mean() * 100\n    mean_activation = np.abs(latent_vecs).mean()\n\n    return sparsity, mean_activation\n\nclass LatentVisualizer:\n    def __init__(self, results_folder):\n        self.sparsity_history = []\n        self.activation_history = []\n        self.results_folder = results_folder\n\n    def update(self, latent_vecs) -> None:\n        \"\"\"Update tracking statistics during training.\"\"\"\n        sparsity, mean_activation = track_latent_statistics(latent_vecs)\n        self.sparsity_history.append(sparsity)\n        self.activation_history.append(mean_activation)\n\n    def plot_training_history(self) -> None:\n        \"\"\"Plot the evolution of latent statistics during training.\"\"\"\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n        ax1.plot(self.sparsity_history)\n        ax1.set_title('Latent Vector Sparsity Over Training')\n        ax1.set_ylabel('% Near-Zero Values')\n        ax1.set_xlabel('Training Step')\n\n        ax2.plot(self.activation_history)\n        ax2.set_title('Mean Latent Vector Activation Over Training')\n        ax2.set_ylabel('Mean Absolute Value')\n        ax2.set_xlabel('Training Step')\n\n        plt.tight_layout()\n        plt.savefig(f'{self.results_folder}/latent_statistics.png')\n\n```\nthis is my visualisation code.\n\n\n```py aigs/simple_autoencoder/train.py:192-318\n# Excerpt from: \ng(\"\\nstarting training\", \"TRAIN\")\n    train_mse_losses = []\n    train_spa_losses = []\n    eval_losses = []\n\n    print(\n        \"Train data stats:\", train_loader.min(), train_loader.max(), train_loader.mean()\n    )\n    print(\n        \"Validation data stats:\",\n        validation_loader.min(),\n        validation_loader.max(),\n        validation_loader.mean(),\n    )\n\n    # utils for visaulisation and checkpointing\n    visualizer = LatentVisualizer(config.results_folder)\n    checkpointer = ocp.StandardCheckpointer()\n    train_step_jit = jax.jit(train_step, static_argnums=3)\n    evaluate_fun_jit = jax.jit(evaluate_fun, static_argnums=3)\n\n    # init aim hyperparameters\n    run['hparams'] = {\n        'batch_size': config.batch_size,\n        'learning_rate': config.learning_rate,\n        'latent_dim': config.latent_dim,\n        'sparsity': config.sparsity,\n        'l1': config.l1,\n        'num_epochs': config.num_epochs,\n        'subject': config.subject,\n        'hemisphere': config.hem,\n        'roi_class': config.roi_class,\n    }\n\n    for epoch in range(config.num_epochs):\n        rng, epoch_key = jax.random.split(rng)\n        validation_step = 0\n        val_loss = 100  # only used for initial tqdm printing\n\n        # im reshuffling also the first time, which is useless but I think the code is cleaner\n        key1, key2 = random.split(rng)\n        train_loader = get_batches(train_ds, key1, config.batch_size)\n        validation_loader = get_batches(validation_ds, key2, config.batch_size)\n\n        # pre_op = jax.jit(sparsity_updater.pre_forward_update)\n        post_op = jax.jit(sparsity_updater.post_gradient_update)\n\n        tmp_loss = []\n\n        # Training loop\n        for step in (\n            pbar := tqdm(\n                range(0, len(train_loader), config.batch_size), total=steps_per_epoch\n            )\n        ):\n            batch = train_loader[step : step + config.batch_size]\n            state, losses = train_step_jit(state, batch, epoch_key, config)\n            mse_loss, spa_loss = losses[\"mse_loss\"], losses[\"sparsity_loss\"]\n\n            # implemening sparsity\n            post_params = post_op(state.params, state.opt_state)\n            state = state.replace(params=post_params)\n\n            tmp_loss.append(mse_loss)\n            # train_mse_losses.append(mse_loss)\n            # train_spa_losses.append(spa_loss)\n\n            if step % (steps_per_epoch // 5) == 0:\n                validation_batch = validation_loader[\n                    validation_step : validation_step + config.batch_size\n                ]\n                validation_step = +config.batch_size\n\n                metrics, (evaluated_batches, reconstructions), latent_vecs = (\n                    evaluate_fun_jit(state, validation_batch, epoch_key, config)\n                )\n\n                visualizer.update(latent_vecs)\n\n                # average the training loss and append it to the list\n                train_mse_losses.append(jnp.mean(jnp.array(tmp_loss)))\n                run.track(jnp.mean(jnp.array(tmp_loss)), name='train_loss')\n                run.track(metrics['mse_loss'], name='validation_loss')\n                tmp_loss = []\n\n                eval_losses.append(metrics[\"mse_loss\"])\n                val_loss = metrics[\"mse_loss\"]\n                # print(jaxpruner.summarize_sparsity(\n                #             state.params, only_total_sparsity=True))\n\n            pbar.set_description(\n                f\"epoch {epoch} mse loss: {str(mse_loss)[:5]} spa loss: {str(spa_loss)[:5]} val_loss: {str(val_loss)[:5]}\"\n            )\n\n        # once every 10 epochs, plot the original and reconstructed images of the last batch\n        if epoch % rate_reconstruction_printing == 0:\n            validation_batch = validation_loader[\n                validation_step : validation_step + config.batch_size\n            ]\n            validation_step = +config.batch_size\n            metrics, (evaluated_batches, reconstructions), latent_vecs = evaluate_fun(\n                state, validation_batch, epoch_key, config\n            )\n\n            # plot_original_reconstruction(evaluated_batches, reconstructions, config, epoch)\n            # visualize_latent_activations(latent_vecs, evaluated_batches, config.results_folder,epoch)\n            # plot_latent_heatmap(latent_vecs, evaluated_batches, config.results_folder,epoch)\n\n    # save model to disk TODO: use os for this!!\n    ckpt_folder = ocp.test_utils.erase_and_create_empty(\n        f\"{PROJECT_DIR}/{config.results_folder}/checkpoints\"\n    )\n    checkpointer.save(ckpt_folder / \"final\", state)\n    plot_losses(\n        train_mse_losses,\n        train_spa_losses,\n        config.results_folder,\n        eval_losses,\n        steps_per_epoch,\n    )\n    plot_original_reconstruction_fmri(config.subject, evaluated_batches, reconstructions, config.hem)\n    visualize_latent_activations(\n        latent_vecs, evaluated_batches, config, epoch\n    )\n    plot_latent_heatmap(latent_vecs, evaluated_batches, config, epoch)\n    visualizer.plot_training_history()\n\n```\nthis is my training code.\nI get this error trying to visualise thge fmri reconstruction,in the end:     fsaverage_response = map_fsaverage_resp(subj, fmri, img, CLASS_TO_ROI[roi_class][0], hemi, full_class=True)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/andrea/Desktop/aigs/simple_autoencoder/surf_plot.py\", line 93, in map_fsaverage_resp\nfsaverage_response[np.where(fsvg_roi)[0]] = fmri[img, np.where(ch_roi)[0]]\n                                            ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nIndexError: index 2 is out of bounds for axis 1 with size 0\n(.venv) \nThe error you're encountering, `IndexError: index 2 is out of bounds for axis 1 with size 0`, suggests that the `fmri` array does not have the expected dimensions or that the indices being used to access it are out of range. This typically happens when the data is not loaded correctly or the indices are not properly calculated.\n\nLet's break down the potential causes and solutions:\n\n1. **Check the Dimensions of `fmri`**:\n   Ensure that the `fmri` array has the correct dimensions. It should be a 2D array where the first dimension corresponds to the number of images and the second dimension corresponds to the number of voxels.\n\n2. **Verify the Indices**:\n   Ensure that the indices used to access the `fmri` array are within the valid range. Specifically, check the values of `img` and `np.where(ch_roi)[0]`.\n\n3. **Debugging Information**:\n   Add some print statements to check the shapes and values of the variables involved in the indexing operation.\n\nHere is a modified version of the `map_fsaverage_resp` function with additional print statements for debugging:\n\n```python\ndef map_fsaverage_resp(subj, fmri, img, roi, hemisphere: str, full_class=False):\n    challenge_roi_class, fsaverage_roi_class, roi_map = get_roi_data(subj, ROI_TO_CLASS[roi], hemisphere)\n    if full_class:\n        fsvg_roi, ch_roi = fsaverage_roi_class, challenge_roi_class\n    else:\n        # Select the vertices corresponding to the ROI of interest\n        roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)] # the id of the roi\n        challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int) # set all other roi ids to 0 (False)\n        fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n        fsvg_roi, ch_roi = fsaverage_roi, challenge_roi\n\n    # Debugging information\n    print(f\"fmri shape: {fmri.shape}\")\n    print(f\"img index: {img}\")\n    print(f\"fsvg_roi shape: {fsvg_roi.shape}\")\n    print(f\"ch_roi shape: {ch_roi.shape}\")\n    print(f\"np.where(fsvg_roi)[0]: {np.where(fsvg_roi)[0]}\")\n    print(f\"np.where(ch_roi)[0]: {np.where(ch_roi)[0]}\")\n\n    fsaverage_response = np.zeros(len(fsvg_roi))\n    fsaverage_response[np.where(fsvg_roi)[0]] = fmri[img, np.where(ch_roi)[0]]\n    return fsaverage_response\n```\n\nRun your code again and check the output of the print statements. This will help you identify if the shapes and indices are as expected.\n\n4. **Check Data Loading**:\n   Ensure that the `fmri` data is loaded correctly and that it contains the expected number of images and voxels. Verify that the `SUBJECTS` dictionary is populated correctly.\n\n5. **Check the `unmask_from_roi_class` Function**:\n   Ensure that the `unmask_from_roi_class` function is correctly unmasking the data and returning the expected shape.\n\nBy following these steps, you should be able to identify the root cause of the `IndexError` and fix the issue. If the problem persists, please provide the output of the print statements for further analysis.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":131}}},{"id":{"replica_id":0,"value":22},"start":8836,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":132}}},{"id":{"replica_id":0,"value":67},"start":25467,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":133}}},{"id":{"replica_id":0,"value":68},"start":28446,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":134}}}],"summary":"fMRI Reconstruction Visualization Error","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}