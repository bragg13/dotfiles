{"id":"18db5ddf-8303-4acb-a650-c87a197bbc66","zed":"context","version":"0.4.0","text":"\n\n\n\n```py aigs/simple_autoencoder/train.py:1-231\n# Excerpt from: \n\"\"\"Training and evaluation logic.\"\"\"\nimport matplotlib.pyplot as plt\nfrom flax import linen as nn\nimport models\nfrom logger import log\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\nimport optax\nfrom tqdm import tqdm\nfrom nsd_data import get_train_test_datasets, get_batches\nfrom visualisations import plot_losses\nfrom flax.training import train_state\nfrom typing import Any\n\nclass TrainState(train_state.TrainState):\n    batch_stats: Any\n\ndef create_train_state(key, init_data, latent_dim, fmri_voxels, learning_rate):\n    \"\"\"Creates initial `TrainState`.\"\"\"\n    model_cls = models.model(latent_dim, fmri_voxels)\n    variables = model_cls.init(\n        {'params': key, 'dropout': key},\n        init_data,\n        dropout_rng=key,\n        training=True\n    )\n    return TrainState.create(\n        apply_fn=model_cls.apply,\n        params=variables['params'],\n        batch_stats=variables['batch_stats'],\n        tx=optax.adamw(learning_rate)\n    )\n\ndef compute_metrics(recon_x, x, latent_vec):\n    mse_loss = jnp.mean(jnp.square(recon_x - x))\n    l2_loss = jnp.sum(jnp.square(latent_vec))\n    return {'loss': mse_loss + l2_loss }\n\ndef train_step(state, batch, key, latent_dim, l1_coefficient=0.01):\n    key, dropout_rng = jax.random.split(key)\n    \"\"\"Performs a single training step updating model parameters based on batch data.\n\n    Args:\n        state: TrainState object containing model state and optimizer\n        batch: Current batch of training data\n        key: Random key for generating latent vectors\n        latent_dim: Dimension of the latent space\n        l1_coefficient: Optional coefficient for L1 regularization (default 0.01)\n\n    Returns:\n        Tuple containing updated state and training loss for the batch\n    \"\"\"\n    def loss_fn(params):\n        fmri_voxels = batch.shape[1]\n        variables = {'params': params, 'batch_stats': state.batch_stats}\n        (recon_x, latent_vec), new_model_state = models.model(latent_dim, fmri_voxels).apply(\n            variables, batch, dropout_rng, training=True, mutable=['batch_stats'], rngs={'dropout': dropout_rng}\n        )\n        mse_loss = jnp.mean(jnp.square(recon_x - batch))\n        l2_loss = jnp.sum(jnp.square(latent_vec))\n        # sparsity_loss = l1_coefficient * jnp.sum(latent_vec) # jnp.sum(jnp.abs(latent_vec))\n\n        return mse_loss+l2_loss, new_model_state\n\n    (loss, new_model_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    state = state.apply_gradients(grads=grads)\n    state = state.replace(batch_stats=new_model_state['batch_stats'])\n    return state, loss\n\n\ndef evaluate_fun(state, loader, key, latent_dim, num_steps=2):\n    key, dropout_rng = jax.random.split(key)\n    \"\"\"Evaluation function that computes metrics on batches.\n       Evaluates 2 batches by default (during training),\n       while evaluating the entire dataset during testing.\n\n    Args:\n        params: Model parameters to evaluate\n        train_loader: Generator yielding training batches\n        key: Random key for generating latent vectors\n        latent_dim: Dimension of the latent space\n        num_steps: Number of evaluation steps to perform\n\n    Returns:\n        List of tuples containing (metrics, data, latent vectors) for each evaluation step\n        Average of the losses\n    \"\"\"\n    def eval_model(batch):\n        variables = {'params': state.params, 'batch_stats': state.batch_stats}\n        reconstruction, latent_vecs = models.model(latent_dim, batch.shape[1]).apply(\n            variables, batch, dropout_rng=dropout_rng, training=False, mutable=False)\n        metrics = compute_metrics(reconstruction, batch, latent_vecs)\n        return metrics, (batch, reconstruction), latent_vecs\n\n    evaluations = []\n    for j in range(num_steps):\n        print(j)\n        batch = next(loader)\n        # fmri_voxels = batch.shape[1]\n        eval = eval_model(batch)\n        evaluations.append(eval)\n    return evaluations\n\ndef train_and_evaluate(config):\n    \"\"\"Train and evaulate pipeline.\"\"\"\n    rng = random.key(0)\n    rng, init_key = random.split(rng)\n\n    log('Initializing dataset...', 'TRAIN')\n    train_ds, test_ds = get_train_test_datasets(subject=3, roi_class='floc-bodies', hem='lh')\n    print(f\"training ds shape: {train_ds.shape}\")\n    print(f\"test ds shape: {test_ds.shape}\")\n    train_loader = get_batches(train_ds, config.batch_size)\n    test_loader = get_batches(test_ds, config.batch_size)\n    train_size = train_ds.shape[0]\n    test_size = test_ds.shape[0]\n    fmri_voxels = train_ds.shape[1]\n\n\n    log('Initializing model...', 'TRAIN')\n    # va bene che siano uni o random gaussian noise?\n    init_data = jnp.ones((config.batch_size, fmri_voxels), jnp.float32)\n\n    log('Initializing params...', 'TRAIN')\n    # params = models.model(config.latent_dim, fmri_voxels).init({'params': key, 'dropout': key}, init_data, rng, training=False)['params']\n\n    log('Initializing state...', 'TRAIN')\n    state = create_train_state(init_key, init_data, config.latent_dim, fmri_voxels, config.learning_rate)\n    # state = train_state.TrainState.create(\n    #     apply_fn=models.model(config.latent_dim, fmri_voxels).apply,\n    #     params=params,\n    #     tx=optax.adamw(config.learning_rate),\n    # )\n\n    # rng, z_key, eval_rng = random.split(rng, 3)\n\n    log(f'Calculating training steps per epochs (train_size: {train_size})...', 'TRAIN')\n    steps_per_epoch = train_size // int(config.batch_size)\n    if train_size % int(config.batch_size) != 0:\n        steps_per_epoch += 1\n    log(f\"{steps_per_epoch} steps for each ({config.num_epochs}) epoch\", 'TRAIN')\n\n\n\n    log('\\nstarting training', 'TRAIN')\n    train_losses = []\n    eval_losses = []\n    test_losses = []\n\n    for epoch in range(config.num_epochs):\n        # log(f'Epoch {epoch + 1}/{config.num_epochs}', 'TRAIN LOOP')\n        rng, epoch_key = jax.random.split(rng)\n        epoch_train_losses = []\n        epoch_eval_losses = []\n\n        # Training loop\n        for step, batch in (pbar := tqdm(enumerate(train_loader), total=steps_per_epoch)):\n            if step >= steps_per_epoch:\n                break\n\n            state, loss = train_step(state, batch, epoch_key, config.latent_dim)\n            epoch_train_losses.append(loss)\n\n            # every 25% of steps for this epoch, evaluate the model\n            if step % (steps_per_epoch // 2) == 0:\n                rng, key = jax.random.split(rng)\n                evaluations = evaluate_fun(\n                    state, train_loader, epoch_key, config.latent_dim, num_steps=5\n                )\n\n                # calculate average loss among evaluations\n                eval_loss = sum([eval[0]['loss'] for eval in evaluations]) / len(evaluations)\n                epoch_eval_losses.append(eval_loss)\n\n                # only plotting the first evaluation\n                metrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[0]\n                fig,axs = plt.subplots(5, 3, figsize=(15,15))\n                fig.suptitle('Validation set', fontsize=14)\n                axs[0, 0].set_title('original')\n                axs[0, 1].set_title('reconstructed')\n                axs[0, 2].set_title('difference')\n\n                # plot the first 5 samples of the first batch evaluated\n                for i in range(5):\n                    original = evaluated_batches[i][:3600].reshape(36, 100)\n                    reconstruction = reconstructions[i][:3600].reshape(36, 100)\n                    axs[i, 0].imshow(original, cmap='gray')\n                    axs[i, 1].imshow(reconstruction, cmap='gray')\n                    axs[i, 2].imshow(original - reconstruction, cmap='gray')\n\n                # should I append the eval loss to the training losses?\n                # train_losses.append(metrics['loss'])\n                fig.savefig(f'./results/epoch_{epoch}_step_{step}.png')\n                plt.close()\n\n            pbar.set_description(f\"step {step} loss: {str(loss)[:5]}\")\n        train_losses.append(sum(epoch_train_losses)/len(epoch_train_losses))\n        eval_losses.append(sum(epoch_eval_losses)/len(epoch_eval_losses))\n\n\n\n    # evaluate on the test set\n    rng, key = jax.random.split(rng)\n    evaluations = evaluate_fun(state, test_loader, key, config.latent_dim,\n        num_steps=test_size // config.batch_size\n)\n    # evaluations = evaluate_fun(\n    #     state.params, test_loader, key, config.latent_dim, num_steps=test_size // config.batch_size\n    # )\n    metrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[0]\n    test_losses = [eval[0]['loss'] for eval in evaluations]\n\n    # plotting only the first 5 samples of the first batch evaluated\n    fig, axs = plt.subplots(5, 3, figsize=(15,15))\n    fig.suptitle('Testing set', fontsize=14)\n    axs[0, 0].set_title('original')\n    axs[0, 1].set_title('reconstructed')\n    axs[0, 2].set_title('difference')\n    for i in range(5):\n        original = evaluated_batches[i][:3600].reshape(36, 100)\n        reconstruction = reconstructions[i][:3600].reshape(36, 100)\n        axs[i, 0].imshow(original, cmap='gray')\n        axs[i, 1].imshow(reconstruction, cmap='gray')\n        axs[i, 2].imshow(original - reconstruction, cmap='gray')\n\n    fig.savefig('./results/testing.png')\n    plt.close()\n\n    plot_losses(train_losses, eval_losses, test_losses)\n\n```\nis my way of testing correct in the end of the file? im only evaluating the first batch of the test set.\n\nIn your current implementation, you're not evaluating just the first batch of the test set. Let's look at the relevant part:\n\n```python\nevaluations = evaluate_fun(state, test_loader, key, config.latent_dim,\n    num_steps=test_size // config.batch_size\n)\n```\n\nYou're actually evaluating `test_size // config.batch_size` number of batches, which should cover most of your test set (except potentially the last incomplete batch if the test size isn't perfectly divisible by the batch size).\n\nHowever, when you're plotting the results:\n\n```python\nmetrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[0]\n```\n\nYou're only visualizing the first batch's results (`evaluations[0]`), even though you've evaluated multiple batches.\n\nFor the loss calculation, you're correctly using all evaluated batches:\n\n```python\ntest_losses = [eval[0]['loss'] for eval in evaluations]\n```\n\nTo make the visualization more representative of the entire test set, you could:\n\n1. Either randomly sample batches to visualize from all evaluations:\n```python\n# Randomly select a batch to visualize\nimport random\nrandom_batch_idx = random.randint(0, len(evaluations)-1)\nmetrics, (evaluated_batches, reconstructions), latent_vecs = evaluations[random_batch_idx]\n```\n\n2. Or visualize samples from multiple different batches:\n```python\n# Create a larger figure to show samples from multiple batches\nfig, axs = plt.subplots(5, 3, figsize=(15,15))\nfor i in range(5):\n    batch_idx = i % len(evaluations)\n    sample_idx = i // len(evaluations)\n    _, (evaluated_batches, reconstructions), _ = evaluations[batch_idx]\n    original = evaluated_batches[sample_idx][:3600].reshape(36, 100)\n    reconstruction = reconstructions[sample_idx][:3600].reshape(36, 100)\n    # ... rest of your plotting code\n```\n\nThis would give you a better overview of how your model performs across different parts of the test set, not just on the first batch.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":490}}},{"id":{"replica_id":0,"value":39},"start":9443,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":491}}},{"id":{"replica_id":0,"value":40},"start":11358,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":492}}}],"summary":"Test Evaluation Loop Implementation Issue","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}