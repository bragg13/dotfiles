{"id":"fa71a708-afbf-406e-818f-b99e7a05dde0","zed":"context","version":"0.4.0","text":"\n\n\n\n```py neuroscope/src/fmri.py:1-78\n# Excerpt from: \n# fmri.py\n#      neuroscope project\n# by: Noah Syrkis\n\n# imports\nimport os\nimport numpy as np\nfrom nilearn import datasets\nfrom nilearn.surface import load_surf_mesh\nfrom src.utils import SUBJECTS, ROI_TO_CLASS, load_roi_data\n\n\n# globals\nROI_DATA = {subject: load_roi_data(subject) for subject in SUBJECTS}\nATLAS = datasets.fetch_surf_fsaverage(\"fsaverage\")\n\n\n# functions\ndef get_space_mappings(subject: str, hem: str, roi: str) -> np.ndarray:\n    \"\"\"return the fsaverage space mapping for the given subject and hemisphere\"\"\"\n    if not roi:\n        fsaverage_space = ROI_DATA[subject]['fsaverage'][hem]['all-vertices']\n        return fsaverage_space\n    else:\n        fsaverage_space = ROI_DATA[subject]['fsaverage'][hem][ROI_TO_CLASS[roi]]\n        challenge_space = ROI_DATA[subject]['challenge'][hem][ROI_TO_CLASS[roi]]\n        roi_id = ROI_DATA[subject]['mapping'][ROI_TO_CLASS[roi]]['roi_to_id'][roi]\n        fsaverage_space = np.asarray(fsaverage_space == roi_id, dtype=int)\n        challenge_space = np.asarray(challenge_space == roi_id, dtype=int)\n        return fsaverage_space, challenge_space\n\n\ndef fsaverage_vec(challenge_vec, subject, roi, hem) -> np.ndarray:\n    \"\"\"convert a challenge vector to fsaverage space\"\"\"\n    if roi:\n        fsaverage_space, challenge_space = get_space_mappings(subject, hem, roi)\n        fsaverage_response = np.zeros(len(fsaverage_space))\n        fsaverage_response[np.where(fsaverage_space)[0]] = \\\n        challenge_vec[np.where(challenge_space)[0]]\n    else:\n        fsaverage_space = get_space_mappings(subject, hem, roi)\n        fsaverage_response = np.zeros(len(fsaverage_space))\n        fsaverage_response[np.where(fsaverage_space)[0]] = challenge_vec\n    return fsaverage_response\n\nimport numpy as np\n\ndef get_bold_with_coords_and_faces(challenge_vec, subject, hem, roi=None):\n    \"\"\"\n    Return the BOLD signal data along with corresponding coordinates and faces on the brain surface.\n    \"\"\"\n    # Get the fsaverage vector corresponding to the challenge_vec\n    fsaverage_response = fsaverage_vec(challenge_vec, subject, roi, hem)\n\n    # Determine the hemisphere to select the correct mesh\n    side = \"infl_left\" if hem == \"lh\" else \"infl_right\"\n\n    # Load the coordinates and faces for the selected hemisphere\n    coords, faces = load_surf_mesh(ATLAS[side])\n\n    # Create a mask for non-zero fsaverage_response\n    response_mask = np.where(fsaverage_response)[0]\n\n    # Filter the coordinates\n    filtered_coords = coords[response_mask]\n\n    # Create a mapping from old vertex indices to new ones\n    index_mapping = np.full(np.max(faces) + 1, -1)  # Initialize with -1\n    index_mapping[response_mask] = np.arange(response_mask.size)\n\n    # Adjust faces to new indexing and filter out invalid faces\n    filtered_faces = index_mapping[faces]\n    valid_faces_mask = np.all(filtered_faces != -1, axis=1)\n    filtered_faces = filtered_faces[valid_faces_mask]\n\n    # Return the filtered coordinates, corresponding BOLD signal values, and adjusted faces\n    return filtered_coords, fsaverage_response[response_mask], filtered_faces\n\n```\n\n```py neuroscope/src/model.py:1-55\n# Excerpt from: \n# model.py\n#     neuroscope project\n# by: Noah Syrkis\n\n# imports\nimport jax\nfrom jax import numpy as jnp\nimport haiku as hk\nfrom typing import Optional\nfrom functools import partial\nfrom typing import Optional\nfrom src.utils import CONFIG\n\n\n# Define your function\ndef network_fn(fmri: jnp.ndarray,  dropout_rate: Optional[float] = None) -> jnp.ndarray:\n    fc_layer = [\n        hk.Linear(512), jax.nn.gelu,\n        hk.Linear(512), jax.nn.gelu,\n        hk.Linear(CONFIG['image_size'] * CONFIG['image_size'] * 3)\n        ]\n\n    conv_layer = [\n        hk.Conv2D(64, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(16, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(8, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n        hk.Conv2D(3, kernel_shape=3, stride=1, padding='SAME'), jax.nn.gelu,\n    ]\n\n    # apply fc and reshape to image\n    z = hk.Sequential(fc_layer)(fmri)\n\n    # apply dropout if training\n    if dropout_rate is not None:\n        rng = hk.next_rng_key()\n        z = hk.dropout(rng, dropout_rate, z)\n\n    # reshape to image\n    z = z.reshape(-1, CONFIG['image_size'], CONFIG['image_size'], 3)\n    z = hk.Sequential(conv_layer)(z)\n    z = jax.nn.sigmoid(z)\n    return z\n\n\ndef loss_fn(params: hk.Params, rng: jnp.ndarray, fmri: jnp.ndarray, img: jnp.ndarray, dropout_rate: Optional[float] = None) -> jnp.ndarray:\n    pred = apply(params, rng, fmri, dropout_rate=dropout_rate)\n    loss = jnp.mean((pred - img) ** 2)\n    return loss\n\n\n# Create your partial function\ninit, apply = hk.transform(network_fn)\n```\n\n```py neuroscope/src/data.py:1-125\n# Excerpt from: \n# data.py\n#     neuroscope project\n# by: Noah Syrkis\n\n# imports\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nimport cv2\nimport jax.numpy as jnp\nimport pickle\nfrom src.utils import DATA_DIR, get_metadata_sources\n\n\n# functions\ndef load_subjects(subjects: list, image_size: int=32, precision=jnp.float32) -> dict:\n    return {subject: load_subject(subject, image_size, precision) for subject in subjects}\n    \n\ndef load_subject(subject: str, image_size: int=32, precision=jnp.float32) -> tuple:\n    path = os.path.join(DATA_DIR, 'algonauts', subject, 'training_split')\n    n_samples = len([f for f in os.listdir(os.path.join(path, 'training_images')) if f.endswith('.png')])\n    train_idx, _ = train_test_split(np.arange(n_samples), test_size=0.2, random_state=42)\n    return load_split(path, train_idx, image_size, subject, precision=precision)\n\n\ndef load_split(path: str, split_idx: int, image_size: int, subject: str, precision) -> tuple:\n    lh_fmri = np.load(os.path.join(path, 'training_fmri', 'lh_training_fmri.npy'))[split_idx]\n    rh_fmri = np.load(os.path.join(path, 'training_fmri', 'rh_training_fmri.npy'))[split_idx]\n    images, metadata = load_coco(path, split_idx, image_size, subject)\n    return jnp.array(lh_fmri), jnp.array(rh_fmri), jnp.array(images) # , metadata\n\n\ndef load_metadata(image_files: list, subject: str) -> list:\n    metadata_cache = os.path.join(DATA_DIR, 'cache', f'metadata_{subject}.pkl')\n    if os.path.exists(metadata_cache):\n        with open(metadata_cache, 'rb') as f:\n            metadata = pickle.load(f)\n    else:\n        metadata_sources = get_metadata_sources()\n        metadata = [get_metadata(image_file, metadata_sources) for image_file in image_files]\n        with open(metadata_cache, 'wb') as f:\n            pickle.dump(metadata, f)\n    return metadata\n\n\ndef load_images(image_files: list, path: str, image_size: int, subject: str) -> np.ndarray:\n    image_cache = os.path.join(DATA_DIR, 'cache', f'images_{subject}_{image_size}.npy')\n    if os.path.exists(image_cache):\n        images = np.load(image_cache)\n    else:\n        load_image = lambda f: cv2.resize(cv2.cvtColor(cv2.imread(f, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB), (image_size, image_size)) / 255.\n        image_paths = [os.path.join(path, 'training_images', f) for f in image_files]\n        images = np.array([load_image(image_path) for image_path in tqdm(image_paths)])\n        np.save(image_cache, images)\n    return images\n\n\ndef load_coco(path: str, split_idx: int, image_size: int, subject: str) -> tuple:\n    image_files = sorted([f for f in os.listdir(os.path.join(path, 'training_images')) if f.endswith('.png')])\n    image_files = [image_files[i] for i in split_idx]\n    metadata = load_metadata(image_files, subject)\n    images = load_images(image_files, path, image_size, subject)\n    return images, metadata\n\n\ndef get_metadata(image_file: str, metadata_sources: tuple) -> tuple:\n    coco_instances, coco_captions, nsd_stim_info = metadata_sources\n    nsd_id = int(image_file.split('_')[1].split('.')[0].split('-')[1])\n    coco_id = nsd_stim_info.loc[nsd_stim_info['nsdId'] == nsd_id, 'cocoId'].iloc[0]\n    annotation_ids = coco_instances.getAnnIds(imgIds=coco_id)\n    annotations = coco_instances.loadAnns(annotation_ids)\n    category_ids = [annotation['category_id'] for annotation in annotations]\n    categories = [coco_instances.loadCats(category_id)[0]['name'] for category_id in category_ids]\n    caption_ids = coco_captions.getAnnIds(imgIds=coco_id)\n    captions = [annotation['caption'] for annotation in coco_captions.loadAnns(caption_ids)]\n    return coco_id, categories, captions, category_ids\n\n\ndef make_batches(lh_fmri: list, rh_fmri: list, images: list , subject_idx, batch_size: int, n_subjects: int) -> tuple:\n    lh_fmri = np.array(lh_fmri)\n    rh_fmri = np.array(rh_fmri)\n    images = np.array(images)\n    while True:\n        perm = np.random.permutation(len(images) // batch_size * batch_size)\n        for i in range(0, len(perm), batch_size):\n            batch_perm = perm[i:i + batch_size]\n            lh_batch = lh_fmri[batch_perm]\n            #expanded_lh = expand(lh_batch, subject_idx[batch_perm], n_subjects)\n            rh_batch = rh_fmri[batch_perm]\n            #expanded_rh = expand(rh_batch, subject_idx[batch_perm], n_subjects)\n            image_batch = images[batch_perm]\n            yield lh_batch, rh_batch, image_batch, subject_idx[batch_perm]\n\n\"\"\" def expand(A, v, k):\n    N, M = A.shape\n    B = jnp.zeros((N, M, k))\n    mask = jnp.arange(k) == v[:, None]\n    B = jnp.where(mask[:, None, :], A[:, :, None], B)\n    return B \"\"\"\n\ndef combine_subjects(subjects, cfg):\n    # subjects is a dict of (lh_fmri, rh_fmri, images) tuples\n    lh_fmri = np.concatenate([subject[0] for subject in subjects.values()])\n    rh_fmri = np.concatenate([subject[1] for subject in subjects.values()])\n    images = np.concatenate([subject[2] for subject in subjects.values()])\n    subject_idx = np.concatenate([np.ones(len(subject[0])) * i for i, subject in enumerate(subjects.values())])\n    return lh_fmri, rh_fmri, images, subject_idx\n\n\ndef make_kfolds(subjects_data, cfg, n_splits=5):\n    # subject data is a dict of (lh_fmri, rh_fmri, images) tuples\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    combined_subjects = combine_subjects(subjects_data, cfg)\n    for fold in kf.split(combined_subjects[0]):\n        train_idx, val_idx = fold\n        train_lh, train_rh = combined_subjects[0][train_idx], combined_subjects[1][train_idx]\n        train_images = combined_subjects[2][train_idx]\n        train_subject_idx = combined_subjects[3][train_idx]\n        val_lh, val_rh = combined_subjects[0][val_idx], combined_subjects[1][val_idx]\n        val_images = combined_subjects[2][val_idx]\n        val_subject_idx = combined_subjects[3][val_idx]\n        train_batches = make_batches(train_lh, train_rh, train_images, train_subject_idx, cfg['batch_size'], len(subjects_data))\n        val_batches = make_batches(val_lh, val_rh, val_images, val_subject_idx, cfg['batch_size'], len(subjects_data))\n        yield train_batches, val_batches\n```\nlook at this code. how are convolutions used in the network and what kind of data are we feeding in?\nLooking at the code, this appears to be a brain-to-image reconstruction model that takes fMRI data as input and attempts to generate corresponding visual images. Here's the breakdown:\n\n1. Input Data:\n- The model takes fMRI (functional magnetic resonance imaging) data from both left hemisphere (lh_fmri) and right hemisphere (rh_fmri) of the brain\n- The fMRI data is paired with corresponding images (from COCO dataset, resized to CONFIG['image_size'] x CONFIG['image_size'])\n- The data is loaded and processed in batches\n\n2. Network Architecture:\nThe network has two main parts:\n\na) Fully Connected Layers:\n```python\nfc_layer = [\n    hk.Linear(512), jax.nn.gelu,\n    hk.Linear(512), jax.nn.gelu,\n    hk.Linear(CONFIG['image_size'] * CONFIG['image_size'] * 3)\n]\n```\n- First processes the fMRI data through dense layers\n- Outputs are reshaped into an image-like format (height × width × 3 channels)\n\nb) Convolutional Layers:\n```python\nconv_layer = [\n    hk.Conv2D(64, kernel_shape=3, stride=1, padding='SAME'), \n    hk.Conv2D(32, kernel_shape=3, stride=1, padding='SAME'),\n    # ... more conv layers ...\n    hk.Conv2D(3, kernel_shape=3, stride=1, padding='SAME')\n]\n```\n- Takes the reshaped output from dense layers\n- Uses several 2D convolution layers with GELU activation\n- Gradually reduces the number of channels from 64 → 32 → 16 → 8 → 3\n- All convolutions use 3×3 kernels with 'SAME' padding to maintain spatial dimensions\n- Final output is processed through sigmoid to generate the final image\n\nThe convolutions are being used here as an image processing/generation mechanism, likely to help create more coherent and spatially consistent images from the initial dense layer output. The model is essentially trying to learn a mapping from brain activity patterns (fMRI) to visual images.\n\n```py neuroscope/src/fmri.py:1-78\n# Excerpt from: \n# fmri.py\n#      neuroscope project\n# by: Noah Syrkis\n\n# imports\nimport os\nimport numpy as np\nfrom nilearn import datasets\nfrom nilearn.surface import load_surf_mesh\nfrom src.utils import SUBJECTS, ROI_TO_CLASS, load_roi_data\n\n\n# globals\nROI_DATA = {subject: load_roi_data(subject) for subject in SUBJECTS}\nATLAS = datasets.fetch_surf_fsaverage(\"fsaverage\")\n\n\n# functions\ndef get_space_mappings(subject: str, hem: str, roi: str) -> np.ndarray:\n    \"\"\"return the fsaverage space mapping for the given subject and hemisphere\"\"\"\n    if not roi:\n        fsaverage_space = ROI_DATA[subject]['fsaverage'][hem]['all-vertices']\n        return fsaverage_space\n    else:\n        fsaverage_space = ROI_DATA[subject]['fsaverage'][hem][ROI_TO_CLASS[roi]]\n        challenge_space = ROI_DATA[subject]['challenge'][hem][ROI_TO_CLASS[roi]]\n        roi_id = ROI_DATA[subject]['mapping'][ROI_TO_CLASS[roi]]['roi_to_id'][roi]\n        fsaverage_space = np.asarray(fsaverage_space == roi_id, dtype=int)\n        challenge_space = np.asarray(challenge_space == roi_id, dtype=int)\n        return fsaverage_space, challenge_space\n\n\ndef fsaverage_vec(challenge_vec, subject, roi, hem) -> np.ndarray:\n    \"\"\"convert a challenge vector to fsaverage space\"\"\"\n    if roi:\n        fsaverage_space, challenge_space = get_space_mappings(subject, hem, roi)\n        fsaverage_response = np.zeros(len(fsaverage_space))\n        fsaverage_response[np.where(fsaverage_space)[0]] = \\\n        challenge_vec[np.where(challenge_space)[0]]\n    else:\n        fsaverage_space = get_space_mappings(subject, hem, roi)\n        fsaverage_response = np.zeros(len(fsaverage_space))\n        fsaverage_response[np.where(fsaverage_space)[0]] = challenge_vec\n    return fsaverage_response\n\nimport numpy as np\n\ndef get_bold_with_coords_and_faces(challenge_vec, subject, hem, roi=None):\n    \"\"\"\n    Return the BOLD signal data along with corresponding coordinates and faces on the brain surface.\n    \"\"\"\n    # Get the fsaverage vector corresponding to the challenge_vec\n    fsaverage_response = fsaverage_vec(challenge_vec, subject, roi, hem)\n\n    # Determine the hemisphere to select the correct mesh\n    side = \"infl_left\" if hem == \"lh\" else \"infl_right\"\n\n    # Load the coordinates and faces for the selected hemisphere\n    coords, faces = load_surf_mesh(ATLAS[side])\n\n    # Create a mask for non-zero fsaverage_response\n    response_mask = np.where(fsaverage_response)[0]\n\n    # Filter the coordinates\n    filtered_coords = coords[response_mask]\n\n    # Create a mapping from old vertex indices to new ones\n    index_mapping = np.full(np.max(faces) + 1, -1)  # Initialize with -1\n    index_mapping[response_mask] = np.arange(response_mask.size)\n\n    # Adjust faces to new indexing and filter out invalid faces\n    filtered_faces = index_mapping[faces]\n    valid_faces_mask = np.all(filtered_faces != -1, axis=1)\n    filtered_faces = filtered_faces[valid_faces_mask]\n\n    # Return the filtered coordinates, corresponding BOLD signal values, and adjusted faces\n    return filtered_coords, fsaverage_response[response_mask], filtered_faces\n\n```\ntake a look again also here. does it really just pass a vector of voxels to some linear layers and then reshape them to an image? is it allowed to do convolution on such a thing? how does it preserve spatial relationships between voxels? does it even matter?\n\nAh, this is a crucial observation! Looking at the fMRI processing code, we can see it's dealing with brain surface data that has spatial relationships:\n\n1. The data comes from a brain surface mesh (coordinates and faces), where:\n- `coords` represents 3D positions on the brain surface\n- `faces` defines how these points connect to form the surface mesh\n- The BOLD signals (brain activity) are mapped to specific locations on this surface\n\n2. The spatial relationship issue:\n- In the current model architecture, this spatial information is lost when the fMRI data is fed directly into linear layers\n- When we reshape the output of linear layers into a 2D image format and apply convolutions, we're creating artificial spatial relationships that don't necessarily correspond to the original brain topology\n\nA potentially better approach might be to:\n- Preserve the mesh structure of the brain data\n- Use graph convolutions or surface convolutions that respect the actual spatial relationships on the brain surface\n- Consider the anatomical organization of visual areas in the brain when processing the signals\n\nSo you raise a valid point - while the current architecture can learn patterns in the data, it's not taking advantage of the brain's actual spatial organization. Using convolutions on arbitrarily reshaped brain activity data isn't necessarily meaningful from a neuroscientific perspective, even if the model might still learn useful mappings through brute force.\n\nFor more neuroanatomically informed processing, you might want to look into:\n- Graph neural networks for brain meshes\n- Surface convolutions that respect cortical topology\n- Hierarchical processing that mirrors the visual cortex organization\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":624}}},{"id":{"replica_id":0,"value":183},"start":11262,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":625}}},{"id":{"replica_id":0,"value":184},"start":13065,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":626}}},{"id":{"replica_id":0,"value":349},"start":16464,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":627}}},{"id":{"replica_id":0,"value":350},"start":18179,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":628}}}],"summary":"FMRI to Image Using CNN Architecture","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null}]}